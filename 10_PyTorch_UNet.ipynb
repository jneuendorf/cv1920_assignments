{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "10_PyTorch_UNet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZalennMQ54nE",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 10 - UNet\n",
        "\n",
        "In this assignement we are going to program our own UNet network (https://arxiv.org/pdf/1505.04597.pdf) which is a simple but powerful one. This network is made to produce a segmentation map. This segmentation map can be a little bit smaller than the true map but keep the same spatial structure. This map however is composed of several layers, one per class. The goal for the network is to activate pixel-wisely a layer if the pixel are representing the object of the layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uarH9tze54nN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "outputId": "c399f677-6248-41a9-8a95-1eb981cb6e12"
      },
      "source": [
        "from IPython.display import Image\n",
        "from IPython.core.display import HTML \n",
        "Image(url= \"https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png\", width=700)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png\" width=\"700\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpZG9dBJ54nj",
        "colab_type": "text"
      },
      "source": [
        "The network look this way. The descending part is simply made out of convolution layer and pooling, easy peasy. This part of the network allow a move from the \"Where?\" information to the \"What?\" information. Then the informations are spatially dilated through a so called \"transpose convolution\" looking like a convoltuion mixed with an inverse pooling and then you convolute. as I sayed above, there is one layer of exit per class, don't trust the drawing, the initial version of this network was only design to say yes or not (That why there is two output layer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Uh9i-W454ns",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "16bd453e-6cde-4dce-f904-862dda399a25"
      },
      "source": [
        "Image(url= \"https://miro.medium.com/max/3200/0*mk6U6zQDuoQLK7Ca\", width=700)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://miro.medium.com/max/3200/0*mk6U6zQDuoQLK7Ca\" width=\"700\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbkcMVll54n8",
        "colab_type": "text"
      },
      "source": [
        "After each big step of convolution, the informations are stacked to the last part of the network (grey arrow) reinjecting this way the \"Where?\" information.\n",
        "\n",
        "# 8.1\n",
        "\n",
        "Yo have to reproduce this network by yourself. The images takken for this work come from the PascalVOC database (http://host.robots.ox.ac.uk/pascal/VOC/). Here you inject RGB images into your network and out a \"cube\" of maps. The label of the data are on the shape of images with one channel, the background is represented by 0 and the differents class by a unique label (all the pixel filled out of ones are representing a plan typically.)\n",
        "\n",
        "You have to use dtype=torch.float32 for the images and dtype=torch.long for the mask and every thing should run perfectly. Use also the criterion to use should be criterion = nn.CrossEntropyLoss() because he can understand the type of label injected (https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss). \n",
        "\n",
        "Try to work on this early, the training can be slow (like 1h for 50 epoch ; batch : 100)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7wuyHJy54n_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bf61c4fa-e90d-4eb3-aeae-8523208d1a74"
      },
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "np.set_printoptions(threshold=sys.maxsize)\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from torch.autograd import Function\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "import time\n",
        "import copy\n",
        "import os\n",
        "\n",
        "torch.__version__"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.3.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AK2yV7C54oL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VOCSegLoader(torchvision.datasets.VOCSegmentation):\n",
        "    def __init__(self, \n",
        "                 root, \n",
        "                 year='2012',\n",
        "                 image_set='train',\n",
        "                 download=False,\n",
        "                 transform=None,\n",
        "                 target_transform=None,\n",
        "                 transforms=None):\n",
        "        \n",
        "        super(VOCSegLoader, self).__init__(root, year, image_set, download, transform, target_transform, transforms)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "\n",
        "        Returns:\n",
        "            tuple: (image, target) where target is the image segmentation.\n",
        "        \"\"\"\n",
        "        img = Image.open(self.images[index]).convert('RGB')\n",
        "        target = Image.open(self.masks[index])\n",
        "        \n",
        "        target = np.array(target)\n",
        "        target[target == 255] = 0\n",
        "        target = Image.fromarray(target)\n",
        "        \n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "        \n",
        "        target = torch.as_tensor(np.asarray(target, dtype=np.uint8), dtype=torch.long)\n",
        "        return img, target          "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjovrkoFgj6y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "05a05c6f-b3d2-48b4-bcce-ef191250f708"
      },
      "source": [
        "n_epochs = 3\n",
        "batch_size = 2\n",
        "#batch_size_train = 100\n",
        "#batch_size_val = 100\n",
        "learning_rate = 0.001\n",
        "momentum = 0.9\n",
        "log_interval = 10\n",
        "image_size = (64, 85)\n",
        "\n",
        "\n",
        "transform_data = torchvision.transforms.Compose([torchvision.transforms.Resize(image_size), \n",
        "                                                 torchvision.transforms.ToTensor()])\n",
        "transform_label = torchvision.transforms.Compose([torchvision.transforms.Resize(image_size, interpolation=0)])\n",
        "\n",
        "\n",
        "\n",
        "image_datasets = {x: VOCSegLoader('./data', year='2012', image_set='train', download=True,\n",
        "                    transform=transform_data, target_transform=transform_label)\n",
        "                  for x in ['train', 'val']}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size = batch_size)\n",
        "              for x in ['train', 'val']}\n",
        "\n",
        "\n",
        "\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using downloaded and verified file: ./data/VOCtrainval_11-May-2012.tar\n",
            "Using downloaded and verified file: ./data/VOCtrainval_11-May-2012.tar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJPC0nGx54og",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "outputId": "5573f145-389c-433f-95e5-886dba9206d1"
      },
      "source": [
        "image, target = image_datasets['train'][2]\n",
        "print(type(image), image.size())\n",
        "print(type(target), target.size())\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(np.asarray(target))\n",
        "plt.show()\n",
        "\n",
        "def imshow(inp, title=None):\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    plt.figure(figsize=(20, 20))\n",
        "    plt.axis('off')\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)\n",
        "\n",
        "def show_databatch(inputs):\n",
        "    out = torchvision.utils.make_grid(inputs)\n",
        "    #print(out.numpy())\n",
        "    imshow(out)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'> torch.Size([3, 64, 85])\n",
            "<class 'torch.Tensor'> torch.Size([64, 85])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUIAAAD7CAYAAAAfH52VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nP5osIKPX3CgaAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_2pTfKw54op",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NETWORK\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 64, 3, padding=1)\n",
        "\n",
        "        self.maxpool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(128, 128, 3, padding=1)\n",
        "\n",
        "        self.conv5 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "        self.conv6 = nn.Conv2d(256, 256, 3, padding=1)\n",
        "\n",
        "        self.conv7 = nn.Conv2d(256, 512, 3, padding=1)\n",
        "        self.conv8 = nn.Conv2d(512, 512, 3, padding=1)\n",
        "\n",
        "        self.conv9 = nn.Conv2d(512, 1024, 3, padding=1)\n",
        "        self.conv10 = nn.Conv2d(1024, 1024, 3, padding=1)\n",
        "\n",
        "        self.upconv1 = nn.ConvTranspose2d(1024, 512, 3, stride=2)\n",
        "        # cat\n",
        "        self.conv11 = nn.Conv2d(1024, 512, 3, padding=1)\n",
        "        self.conv12 = nn.Conv2d(512, 512, 3, padding=1)\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(512, 256, 3, stride=2)\n",
        "        # cat\n",
        "        self.conv13 = nn.Conv2d(512, 256, 3, padding=1)\n",
        "        self.conv14 = nn.Conv2d(256, 256, 3, padding=1)\n",
        "\n",
        "        self.upconv3 = nn.ConvTranspose2d(256, 128, 3, stride=2)\n",
        "        # cat\n",
        "        self.conv15 = nn.Conv2d(256, 128, 3, padding=1)\n",
        "        self.conv16 = nn.Conv2d(128, 128, 3, padding=1)\n",
        "\n",
        "        self.upconv4 = nn.ConvTranspose2d(128, 64, 3, stride=2)\n",
        "        # cat\n",
        "        self.conv17 = nn.Conv2d(128, 64, 3, padding=1)\n",
        "        self.conv18 = nn.Conv2d(64, 64, 3, padding=1)\n",
        "\n",
        "        self.upconvEXTRA = nn.ConvTranspose2d(64, 64, 3, stride=2)\n",
        "\n",
        "        self.conv19 = nn.Conv2d(64, 21, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        a = F.relu(self.conv1(x))\n",
        "        a = self.maxpool(F.relu(self.conv2(a)))\n",
        "\n",
        "        b = F.relu(self.conv3(a))\n",
        "        b = self.maxpool(F.relu(self.conv4(b)))\n",
        "\n",
        "        c = F.relu(self.conv5(b))\n",
        "        c = self.maxpool(F.relu(self.conv6(c)))\n",
        "\n",
        "        d = F.relu(self.conv7(c))\n",
        "        d = self.maxpool(F.relu(self.conv8(d)))\n",
        "\n",
        "        e = F.relu(self.conv9(d))\n",
        "        e = F.relu(self.conv10(e))\n",
        "\n",
        "        f = self.upconv1(e)\n",
        "        #print(d.shape,f.shape)  #torch.Size([100, 512, 3, 3])\n",
        "        f = f[:,:,2:6,2:7]\n",
        "        f = torch.cat((d,f),dim=1)\n",
        "        f = F.relu(self.conv11(f))\n",
        "        f = F.relu(self.conv12(f))\n",
        "\n",
        "        g = self.upconv2(f)\n",
        "        #print(c.shape,g.shape)\n",
        "        g = g[:,:,1:,1:]\n",
        "        g = torch.cat((c,g),dim=1)\n",
        "        g = F.relu(self.conv13(g))\n",
        "        g = F.relu(self.conv14(g))\n",
        "\n",
        "        h = self.upconv3(g)\n",
        "        #print(b.shape,h.shape)\n",
        "        h = h[:,:,1:,:]\n",
        "        h = torch.cat((b,h),dim=1)\n",
        "        h = F.relu(self.conv15(h))\n",
        "        h = F.relu(self.conv16(h))\n",
        "\n",
        "        i = self.upconv4(h)\n",
        "        #print(a.shape,i.shape)\n",
        "        i = i[:,:,1:,1:]\n",
        "        i = torch.cat((a,i),dim=1)\n",
        "        i = F.relu(self.conv17(i))\n",
        "        i = F.relu(self.conv18(i))\n",
        "\n",
        "        j = self.upconvEXTRA(i)\n",
        "        #print(j.shape)\n",
        "        j = j[:,:,1:,:]\n",
        "\n",
        "        k = self.conv19(j)\n",
        "        \n",
        "        return j\n",
        "\n",
        "model = Net()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsdMLBI6eVqi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TRAIN FUNCTION\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLvTjte8fLWh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hclcln0fSvq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "outputId": "f45936c9-3e16-4f71-8706-f34624221cca"
      },
      "source": [
        "model = train_model(model, criterion, optimizer, exp_lr_scheduler,\n",
        "                       num_epochs=20)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "----------\n",
            "train Loss: 2.0183 Acc: 4001.8538\n",
            "val Loss: 1.3999 Acc: 4073.1366\n",
            "\n",
            "Epoch 2/20\n",
            "----------\n",
            "train Loss: 1.3415 Acc: 4073.1366\n",
            "val Loss: 1.3460 Acc: 4073.1366\n",
            "\n",
            "Epoch 3/20\n",
            "----------\n",
            "train Loss: 1.3072 Acc: 4073.1366\n",
            "val Loss: 1.3163 Acc: 4073.1366\n",
            "\n",
            "Epoch 4/20\n",
            "----------\n",
            "train Loss: 1.2836 Acc: 4073.1366\n",
            "val Loss: 1.2945 Acc: 4073.1366\n",
            "\n",
            "Epoch 5/20\n",
            "----------\n",
            "train Loss: 1.2702 Acc: 4073.1366\n",
            "val Loss: 1.2838 Acc: 4073.1366\n",
            "\n",
            "Epoch 6/20\n",
            "----------\n",
            "train Loss: 1.2640 Acc: 4073.1366\n",
            "val Loss: 1.2788 Acc: 4073.1366\n",
            "\n",
            "Epoch 7/20\n",
            "----------\n",
            "train Loss: 1.2604 Acc: 4073.1366\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-111-e72d3207ec9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model = train_model(model, criterion, optimizer, exp_lr_scheduler,\n\u001b[0;32m----> 2\u001b[0;31m                        num_epochs=20)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-109-f2b4d52379ed>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;31m# Iterate over data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-a05bc25a9cec>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mtuple\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mimage\u001b[0m \u001b[0msegmentation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \"\"\"\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \"\"\"\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"P\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3qVjcxb5ypL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# VISUALIZE PREDICTION"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLqDLY7o54o1",
        "colab_type": "text"
      },
      "source": [
        "## 8.2\n",
        "Once you have done that, we want you to redesign a network where you remove to reinjection link (grey arrow on the drawing). You can remove the both from your choice just try and tell us if it's still working and why."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDG04rye54o4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9yesinb54pF",
        "colab_type": "text"
      },
      "source": [
        "## 8.3 BONUSTOCOME"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2H22yFC54pI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
