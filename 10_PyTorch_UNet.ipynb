{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "10_PyTorch_UNet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZalennMQ54nE",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 10 - UNet\n",
        "\n",
        "In this assignement we are going to program our own UNet network (https://arxiv.org/pdf/1505.04597.pdf) which is a simple but powerful one. This network is made to produce a segmentation map. This segmentation map can be a little bit smaller than the true map but keep the same spatial structure. This map however is composed of several layers, one per class. The goal for the network is to activate pixel-wisely a layer if the pixel are representing the object of the layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uarH9tze54nN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "outputId": "c399f677-6248-41a9-8a95-1eb981cb6e12"
      },
      "source": [
        "from IPython.display import Image\n",
        "from IPython.core.display import HTML \n",
        "Image(url= \"https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png\", width=700)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png\" width=\"700\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpZG9dBJ54nj",
        "colab_type": "text"
      },
      "source": [
        "The network look this way. The descending part is simply made out of convolution layer and pooling, easy peasy. This part of the network allow a move from the \"Where?\" information to the \"What?\" information. Then the informations are spatially dilated through a so called \"transpose convolution\" looking like a convoltuion mixed with an inverse pooling and then you convolute. as I sayed above, there is one layer of exit per class, don't trust the drawing, the initial version of this network was only design to say yes or not (That why there is two output layer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Uh9i-W454ns",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "16bd453e-6cde-4dce-f904-862dda399a25"
      },
      "source": [
        "Image(url= \"https://miro.medium.com/max/3200/0*mk6U6zQDuoQLK7Ca\", width=700)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://miro.medium.com/max/3200/0*mk6U6zQDuoQLK7Ca\" width=\"700\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbkcMVll54n8",
        "colab_type": "text"
      },
      "source": [
        "After each big step of convolution, the informations are stacked to the last part of the network (grey arrow) reinjecting this way the \"Where?\" information.\n",
        "\n",
        "# 8.1\n",
        "\n",
        "Yo have to reproduce this network by yourself. The images takken for this work come from the PascalVOC database (http://host.robots.ox.ac.uk/pascal/VOC/). Here you inject RGB images into your network and out a \"cube\" of maps. The label of the data are on the shape of images with one channel, the background is represented by 0 and the differents class by a unique label (all the pixel filled out of ones are representing a plan typically.)\n",
        "\n",
        "You have to use dtype=torch.float32 for the images and dtype=torch.long for the mask and every thing should run perfectly. Use also the criterion to use should be criterion = nn.CrossEntropyLoss() because he can understand the type of label injected (https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss). \n",
        "\n",
        "Try to work on this early, the training can be slow (like 1h for 50 epoch ; batch : 100)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7wuyHJy54n_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bf61c4fa-e90d-4eb3-aeae-8523208d1a74"
      },
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "np.set_printoptions(threshold=sys.maxsize)\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from torch.autograd import Function\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "import time\n",
        "import copy\n",
        "import os\n",
        "\n",
        "torch.__version__"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.3.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AK2yV7C54oL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VOCSegLoader(torchvision.datasets.VOCSegmentation):\n",
        "    def __init__(self, \n",
        "                 root, \n",
        "                 year='2012',\n",
        "                 image_set='train',\n",
        "                 download=False,\n",
        "                 transform=None,\n",
        "                 target_transform=None,\n",
        "                 transforms=None):\n",
        "        \n",
        "        super(VOCSegLoader, self).__init__(root, year, image_set, download, transform, target_transform, transforms)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "\n",
        "        Returns:\n",
        "            tuple: (image, target) where target is the image segmentation.\n",
        "        \"\"\"\n",
        "        img = Image.open(self.images[index]).convert('RGB')\n",
        "        target = Image.open(self.masks[index])\n",
        "        \n",
        "        target = np.array(target)\n",
        "        target[target == 255] = 0\n",
        "        target = Image.fromarray(target)\n",
        "        \n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "        \n",
        "        target = torch.as_tensor(np.asarray(target, dtype=np.uint8), dtype=torch.long)\n",
        "        return img, target          "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjovrkoFgj6y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f39c9334-2d7a-4a6d-d064-a579c304abf1"
      },
      "source": [
        "n_epochs = 3\n",
        "batch_size = 4\n",
        "#batch_size_train = 100\n",
        "#batch_size_val = 100\n",
        "learning_rate = 0.001\n",
        "momentum = 0.9\n",
        "log_interval = 10\n",
        "image_size = (64, 85)\n",
        "\n",
        "\n",
        "transform_data = torchvision.transforms.Compose([torchvision.transforms.Resize(image_size), \n",
        "                                                 torchvision.transforms.ToTensor()])\n",
        "transform_label = torchvision.transforms.Compose([torchvision.transforms.Resize(image_size, interpolation=0)])\n",
        "\n",
        "\n",
        "\n",
        "image_datasets = {x: VOCSegLoader('./data', year='2012', image_set='train', download=True,\n",
        "                    transform=transform_data, target_transform=transform_label)\n",
        "                  for x in ['train', 'val']}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size = batch_size)\n",
        "              for x in ['train', 'val']}\n",
        "\n",
        "\n",
        "\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using downloaded and verified file: ./data/VOCtrainval_11-May-2012.tar\n",
            "Using downloaded and verified file: ./data/VOCtrainval_11-May-2012.tar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJPC0nGx54og",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "outputId": "4c39b2f2-3284-4558-998b-dc68a734df9c"
      },
      "source": [
        "image, target = image_datasets['train'][2]\n",
        "print(type(image), image.size())\n",
        "print(type(target), target.size())\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(np.asarray(target))\n",
        "plt.show()\n",
        "\n",
        "def imshow(inp, title=None):\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    plt.figure(figsize=(20, 20))\n",
        "    plt.axis('off')\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)\n",
        "\n",
        "def show_databatch(inputs):\n",
        "    out = torchvision.utils.make_grid(inputs)\n",
        "    #print(out.numpy())\n",
        "    imshow(out)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'> torch.Size([3, 64, 85])\n",
            "<class 'torch.Tensor'> torch.Size([64, 85])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUIAAAD7CAYAAAAfH52VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nP5osIKPX3CgaAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_2pTfKw54op",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NETWORK\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 64, 3, padding=1)\n",
        "\n",
        "        self.maxpool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(128, 128, 3, padding=1)\n",
        "\n",
        "        self.conv5 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "        self.conv6 = nn.Conv2d(256, 256, 3, padding=1)\n",
        "\n",
        "        self.conv7 = nn.Conv2d(256, 512, 3, padding=1)\n",
        "        self.conv8 = nn.Conv2d(512, 512, 3, padding=1)\n",
        "\n",
        "        self.conv9 = nn.Conv2d(512, 1024, 3, padding=1)\n",
        "        self.conv10 = nn.Conv2d(1024, 1024, 3, padding=1)\n",
        "\n",
        "        self.upconv1 = nn.ConvTranspose2d(1024, 512, 3, stride=2)\n",
        "        # cat\n",
        "        self.conv11 = nn.Conv2d(1024, 512, 3, padding=1)\n",
        "        self.conv12 = nn.Conv2d(512, 512, 3, padding=1)\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(512, 256, 3, stride=2)\n",
        "        # cat\n",
        "        self.conv13 = nn.Conv2d(512, 256, 3, padding=1)\n",
        "        self.conv14 = nn.Conv2d(256, 256, 3, padding=1)\n",
        "\n",
        "        self.upconv3 = nn.ConvTranspose2d(256, 128, 3, stride=2)\n",
        "        # cat\n",
        "        self.conv15 = nn.Conv2d(256, 128, 3, padding=1)\n",
        "        self.conv16 = nn.Conv2d(128, 128, 3, padding=1)\n",
        "\n",
        "        self.upconv4 = nn.ConvTranspose2d(128, 64, 3, stride=2)\n",
        "        # cat\n",
        "        self.conv17 = nn.Conv2d(128, 64, 3, padding=1)\n",
        "        self.conv18 = nn.Conv2d(64, 64, 3, padding=1)\n",
        "\n",
        "        self.upconvEXTRA = nn.ConvTranspose2d(64, 64, 3, stride=2)\n",
        "\n",
        "        self.conv19 = nn.Conv2d(64, 21, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        a = F.relu(self.conv1(x))\n",
        "        a = self.maxpool(F.relu(self.conv2(a)))\n",
        "\n",
        "        b = F.relu(self.conv3(a))\n",
        "        b = self.maxpool(F.relu(self.conv4(b)))\n",
        "\n",
        "        c = F.relu(self.conv5(b))\n",
        "        c = self.maxpool(F.relu(self.conv6(c)))\n",
        "\n",
        "        d = F.relu(self.conv7(c))\n",
        "        d = self.maxpool(F.relu(self.conv8(d)))\n",
        "\n",
        "        e = F.relu(self.conv9(d))\n",
        "        e = F.relu(self.conv10(e))\n",
        "\n",
        "        f = self.upconv1(e)\n",
        "        #print(d.shape,f.shape)  #torch.Size([100, 512, 3, 3])\n",
        "        f = f[:,:,2:6,2:7]\n",
        "        f = torch.cat((d,f),dim=1)\n",
        "        f = F.relu(self.conv11(f))\n",
        "        f = F.relu(self.conv12(f))\n",
        "\n",
        "        g = self.upconv2(f)\n",
        "        #print(c.shape,g.shape)\n",
        "        g = g[:,:,1:,1:]\n",
        "        g = torch.cat((c,g),dim=1)\n",
        "        g = F.relu(self.conv13(g))\n",
        "        g = F.relu(self.conv14(g))\n",
        "\n",
        "        h = self.upconv3(g)\n",
        "        #print(b.shape,h.shape)\n",
        "        h = h[:,:,1:,:]\n",
        "        h = torch.cat((b,h),dim=1)\n",
        "        h = F.relu(self.conv15(h))\n",
        "        h = F.relu(self.conv16(h))\n",
        "\n",
        "        i = self.upconv4(h)\n",
        "        #print(a.shape,i.shape)\n",
        "        i = i[:,:,1:,1:]\n",
        "        i = torch.cat((a,i),dim=1)\n",
        "        i = F.relu(self.conv17(i))\n",
        "        i = F.relu(self.conv18(i))\n",
        "\n",
        "        j = self.upconvEXTRA(i)\n",
        "        #print(j.shape)\n",
        "        j = j[:,:,1:,:]\n",
        "\n",
        "        k = self.conv19(j)\n",
        "        \n",
        "        return j\n",
        "\n",
        "model = Net()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsdMLBI6eVqi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TRAIN FUNCTION\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLvTjte8fLWh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hclcln0fSvq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6043e463-dc80-4623-9092-66dedc47daeb"
      },
      "source": [
        "model = train_model(model, criterion, optimizer, exp_lr_scheduler,\n",
        "                       num_epochs=25)"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "----------\n",
            "train Loss: 1.6039 Acc: 4039.3477\n",
            "val Loss: 1.3526 Acc: 4073.1366\n",
            "\n",
            "Epoch 2/25\n",
            "----------\n",
            "train Loss: 1.3080 Acc: 4073.1366\n",
            "val Loss: 1.3286 Acc: 4073.1366\n",
            "\n",
            "Epoch 3/25\n",
            "----------\n",
            "train Loss: 1.2900 Acc: 4073.1366\n",
            "val Loss: 1.3163 Acc: 4073.1366\n",
            "\n",
            "Epoch 4/25\n",
            "----------\n",
            "train Loss: 1.2708 Acc: 4073.1298\n",
            "val Loss: 1.2741 Acc: 4073.0519\n",
            "\n",
            "Epoch 5/25\n",
            "----------\n",
            "train Loss: 1.2556 Acc: 4073.5301\n",
            "val Loss: 1.2569 Acc: 4067.8941\n",
            "\n",
            "Epoch 6/25\n",
            "----------\n",
            "train Loss: 1.2376 Acc: 4073.6735\n",
            "val Loss: 1.2490 Acc: 4052.1311\n",
            "\n",
            "Epoch 7/25\n",
            "----------\n",
            "train Loss: 1.2194 Acc: 4073.0806\n",
            "val Loss: 1.2265 Acc: 4049.3224\n",
            "\n",
            "Epoch 8/25\n",
            "----------\n",
            "train Loss: 1.1878 Acc: 4070.5471\n",
            "val Loss: 1.1800 Acc: 4071.7507\n",
            "\n",
            "Epoch 9/25\n",
            "----------\n",
            "train Loss: 1.1796 Acc: 4072.1544\n",
            "val Loss: 1.1754 Acc: 4071.9973\n",
            "\n",
            "Epoch 10/25\n",
            "----------\n",
            "train Loss: 1.1760 Acc: 4072.1981\n",
            "val Loss: 1.1726 Acc: 4072.0601\n",
            "\n",
            "Epoch 11/25\n",
            "----------\n",
            "train Loss: 1.1734 Acc: 4072.2295\n",
            "val Loss: 1.1707 Acc: 4072.1318\n",
            "\n",
            "Epoch 12/25\n",
            "----------\n",
            "train Loss: 1.1713 Acc: 4072.2596\n",
            "val Loss: 1.1691 Acc: 4072.1878\n",
            "\n",
            "Epoch 13/25\n",
            "----------\n",
            "train Loss: 1.1694 Acc: 4072.3019\n",
            "val Loss: 1.1678 Acc: 4072.2213\n",
            "\n",
            "Epoch 14/25\n",
            "----------\n",
            "train Loss: 1.1678 Acc: 4072.3367\n",
            "val Loss: 1.1665 Acc: 4072.2452\n",
            "\n",
            "Epoch 15/25\n",
            "----------\n",
            "train Loss: 1.1616 Acc: 4072.3531\n",
            "val Loss: 1.1630 Acc: 4072.2493\n",
            "\n",
            "Epoch 16/25\n",
            "----------\n",
            "train Loss: 1.1609 Acc: 4072.2445\n",
            "val Loss: 1.1625 Acc: 4072.1455\n",
            "\n",
            "Epoch 17/25\n",
            "----------\n",
            "train Loss: 1.1605 Acc: 4072.2097\n",
            "val Loss: 1.1622 Acc: 4072.1059\n",
            "\n",
            "Epoch 18/25\n",
            "----------\n",
            "train Loss: 1.1602 Acc: 4072.1960\n",
            "val Loss: 1.1619 Acc: 4072.0444\n",
            "\n",
            "Epoch 19/25\n",
            "----------\n",
            "train Loss: 1.1599 Acc: 4072.1981\n",
            "val Loss: 1.1617 Acc: 4072.0096\n",
            "\n",
            "Epoch 20/25\n",
            "----------\n",
            "train Loss: 1.1597 Acc: 4072.1714\n",
            "val Loss: 1.1615 Acc: 4071.9884\n",
            "\n",
            "Epoch 21/25\n",
            "----------\n",
            "train Loss: 1.1595 Acc: 4072.1605\n",
            "val Loss: 1.1613 Acc: 4071.9877\n",
            "\n",
            "Epoch 22/25\n",
            "----------\n",
            "train Loss: 1.1602 Acc: 4072.2377\n",
            "val Loss: 1.1595 Acc: 4072.1680\n",
            "\n",
            "Epoch 23/25\n",
            "----------\n",
            "train Loss: 1.1594 Acc: 4072.3005\n",
            "val Loss: 1.1592 Acc: 4072.1926\n",
            "\n",
            "Epoch 24/25\n",
            "----------\n",
            "train Loss: 1.1592 Acc: 4072.2930\n",
            "val Loss: 1.1591 Acc: 4072.1919\n",
            "\n",
            "Epoch 25/25\n",
            "----------\n",
            "train Loss: 1.1592 Acc: 4072.2951\n",
            "val Loss: 1.1591 Acc: 4072.1906\n",
            "\n",
            "Training complete in 14m 35s\n",
            "Best val Acc: 4073.136612\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3qVjcxb5ypL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 806
        },
        "outputId": "e942acea-3951-4434-e9bb-156847bff123"
      },
      "source": [
        "# VISUALIZE PREDICTION\n",
        "def visualize(model):\n",
        "    with torch.no_grad():\n",
        "        for data in dataloaders['val']:\n",
        "            inputs, labels = data\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            \n",
        "            print(predicted)\n",
        "\n",
        "            pred_plot = predicted[0,:,:].cpu().numpy()\n",
        "            plt.imshow(pred_plot)\n",
        "            plt.show()\n",
        "\n",
        "            break\n",
        "    \n",
        "visualize(model)"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUIAAAD7CAYAAAAfH52VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUFLzLEJJzbMIJTXPIpTUPItQUvP+F6qv51rJz48UAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLqDLY7o54o1",
        "colab_type": "text"
      },
      "source": [
        "## 10.2\n",
        "Once you have done that, we want you to redesign a network where you remove to reinjection link (grey arrow on the drawing). You can remove the both from your choice just try and tell us if it's still working and why."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDG04rye54o4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9yesinb54pF",
        "colab_type": "text"
      },
      "source": [
        "## 10.3 BONUSTOCOME"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2H22yFC54pI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
