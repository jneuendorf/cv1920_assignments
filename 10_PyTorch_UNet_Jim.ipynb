{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "10_PyTorch_UNet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZalennMQ54nE",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 10 - UNet\n",
        "\n",
        "In this assignement we are going to program our own UNet network (https://arxiv.org/pdf/1505.04597.pdf) which is a simple but powerful one. This network is made to produce a segmentation map. This segmentation map can be a little bit smaller than the true map but keep the same spatial structure. This map however is composed of several layers, one per class. The goal for the network is to activate pixel-wisely a layer if the pixel are representing the object of the layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uarH9tze54nN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "outputId": "a182e6fb-4000-47f0-fcf2-5f63207d85ad"
      },
      "source": [
        "from IPython.display import Image\n",
        "from IPython.core.display import HTML \n",
        "Image(url= \"https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png\", width=700)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png\" width=\"700\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpZG9dBJ54nj",
        "colab_type": "text"
      },
      "source": [
        "The network look this way. The descending part is simply made out of convolution layer and pooling, easy peasy. This part of the network allow a move from the \"Where?\" information to the \"What?\" information. Then the informations are spatially dilated through a so called \"transpose convolution\" looking like a convoltuion mixed with an inverse pooling and then you convolute. as I sayed above, there is one layer of exit per class, don't trust the drawing, the initial version of this network was only design to say yes or not (That why there is two output layer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Uh9i-W454ns",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "a3cb7d8e-6cdc-479d-88c2-9e0f759aed97"
      },
      "source": [
        "Image(url= \"https://miro.medium.com/max/3200/0*mk6U6zQDuoQLK7Ca\", width=700)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://miro.medium.com/max/3200/0*mk6U6zQDuoQLK7Ca\" width=\"700\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbkcMVll54n8",
        "colab_type": "text"
      },
      "source": [
        "After each big step of convolution, the informations are stacked to the last part of the network (grey arrow) reinjecting this way the \"Where?\" information.\n",
        "\n",
        "# 8.1\n",
        "\n",
        "Yo have to reproduce this network by yourself. The images takken for this work come from the PascalVOC database (http://host.robots.ox.ac.uk/pascal/VOC/). Here you inject RGB images into your network and out a \"cube\" of maps. The label of the data are on the shape of images with one channel, the background is represented by 0 and the differents class by a unique label (all the pixel filled out of ones are representing a plan typically.)\n",
        "\n",
        "You have to use dtype=torch.float32 for the images and dtype=torch.long for the mask and every thing should run perfectly. Use also the criterion to use should be criterion = nn.CrossEntropyLoss() because he can understand the type of label injected (https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss). \n",
        "\n",
        "Try to work on this early, the training can be slow (like 1h for 50 epoch ; batch : 100)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7wuyHJy54n_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "np.set_printoptions(threshold=sys.maxsize)\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from torch.autograd import Function\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AK2yV7C54oL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VOCSegLoader(torchvision.datasets.VOCSegmentation):\n",
        "    def __init__(self, \n",
        "                 root, \n",
        "                 year='2012',\n",
        "                 image_set='train',\n",
        "                 download=False,\n",
        "                 transform=None,\n",
        "                 target_transform=None,\n",
        "                 transforms=None):\n",
        "        \n",
        "        super(VOCSegLoader, self).__init__(root, year, image_set, download, transform, target_transform, transforms)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "\n",
        "        Returns:\n",
        "            tuple: (image, target) where target is the image segmentation.\n",
        "        \"\"\"\n",
        "        img = Image.open(self.images[index]).convert('RGB')\n",
        "        target = Image.open(self.masks[index])\n",
        "        \n",
        "        target = np.array(target)\n",
        "        target[target == 255] = 0\n",
        "        target = Image.fromarray(target)\n",
        "        \n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "        \n",
        "        target = torch.as_tensor(np.asarray(target, dtype=np.uint8), dtype=torch.long)\n",
        "        return img, target          "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8RwKyj454oW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "512f7971-eca4-401b-e90d-1869d7e16136"
      },
      "source": [
        "n_epochs = 3\n",
        "batch_size_train = 100\n",
        "batch_size_val = 100\n",
        "learning_rate = 0.001\n",
        "momentum = 0.9\n",
        "log_interval = 10\n",
        "image_size = (64, 85)\n",
        "\n",
        "\n",
        "transform_data = torchvision.transforms.Compose([torchvision.transforms.Resize(image_size), \n",
        "                                                 torchvision.transforms.ToTensor()])\n",
        "transform_label = torchvision.transforms.Compose([torchvision.transforms.Resize(image_size, interpolation=0)])\n",
        "\n",
        "\n",
        "train_dataset = VOCSegLoader('./data', year='2012', image_set='train', download=True,\n",
        "                                         transform=transform_data, target_transform=transform_label)\n",
        "val_dataset = VOCSegLoader('./data', year='2012', image_set='val', download=True,\n",
        "                                         transform=transform_data , target_transform=transform_label)\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size_train)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size = batch_size_val)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar to ./data/VOCtrainval_11-May-2012.tar\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1999642624it [03:50, 9244077.59it/s]                                "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Using downloaded and verified file: ./data/VOCtrainval_11-May-2012.tar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJPC0nGx54og",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "outputId": "0b624892-fa71-45f3-8e01-49623ebef2a6"
      },
      "source": [
        "image, target = train_dataset[0]\n",
        "print(type(image), image.size())\n",
        "print(type(target), target.size())\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(np.asarray(target))\n",
        "plt.show()\n",
        "\n",
        "def imshow(inp, title=None):\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    plt.figure(figsize=(20, 20))\n",
        "    plt.axis('off')\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)\n",
        "\n",
        "def show_databatch(inputs):\n",
        "    out = torchvision.utils.make_grid(inputs)\n",
        "    #print(out.numpy())\n",
        "    imshow(out)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'> torch.Size([3, 64, 85])\n",
            "<class 'torch.Tensor'> torch.Size([64, 85])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUIAAAD7CAYAAAAfH52VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAPL0lEQVR4nO3db4wdV33G8e9T27GJKSSG1DVx1KQi\nAkVV49BVCAIhSAgEikheoIgUVVZlyW9oG1oqSFqpKlVfgFTx50WFZBGKVdH8IUAdRQgIblDVqgrZ\nEANJTIgJCdhNYqBxSVvJteHXF3dMN9vd7PXunbm7Od+PtLoz597r+Xnv+PE5Z2bupKqQpJb90rQL\nkKRpMwglNc8glNQ8g1BS8wxCSc0zCCU1b0VBmOSqJA8nOZTkhkkVJUlDynLPI0yyDvgucCVwGLgX\nuK6qHppceZLUv/UreO+lwKGqehQgyS3A1cCiQXhGNtYmNq9gk5K0fM/w9I+r6pz57SsJwnOBH85Z\nPwy8+rnesInNvDpXrGCTkrR8X63bH1+ofSVBOJYku4HdAJs4s+/NSdJpW8nBkiPAeXPWt3dtz1JV\ne6pqpqpmNrBxBZuTpH6sJAjvBS5MckGSM4B3AXdMpixJGs6yh8ZVdTLJ7wNfBtYBn6qqBydWmSQN\nZEVzhFX1ReCLE6pFkqbCK0skNc8glNQ8g1BS8wxCSc0zCCU1zyCU1DyDUFLzDEJJzTMIJTXPIJTU\nPINQUvMMQknNMwglNc8glNQ8g1BS8wxCSc0zCCU1zyCU1DyDUFLzDEJJzTMIJTXPIJTUPINQUvMM\nQknNMwglNW/JIEzyqSRHkzwwp21LkruSPNI9nt1vmZLUn3F6hJ8GrprXdgOwv6ouBPZ365K0Ji0Z\nhFX1T8C/z2u+GtjbLe8FrplwXZI0mOXOEW6tqie65SeBrROqR5IGt+KDJVVVQC32fJLdSWaTzJ7g\n+Eo3J0kTt9wgfCrJNoDu8ehiL6yqPVU1U1UzG9i4zM1JUn+WG4R3ADu75Z3AvsmUI0nDG+f0mZuB\nfwVekeRwkl3Ah4ArkzwCvKlbl6Q1af1SL6iq6xZ56ooJ1yJJU+GVJZKaZxBKap5BKKl5BqGk5hmE\nkppnEEpqnkEoqXkGoaTmGYSSmmcQSmqeQSipeQahpOYZhJKaZxBKap5BKKl5BqGk5hmEkppnEEpq\nnkEoqXkGoaTmGYSSmmcQSmqeQSipeQahpOYZhJKat2QQJjkvyd1JHkryYJLru/YtSe5K8kj3eHb/\n5UrS5I3TIzwJvK+qLgIuA96T5CLgBmB/VV0I7O/WJWnNWb/UC6rqCeCJbvmZJAeBc4GrgTd0L9sL\nfA34QC9VamrW/+rWXyyffPKpKVYi9ee05giTnA9cAtwDbO1CEuBJYOsib5OkVW3sIEzyQuBzwHur\n6qdzn6uqAmqR9+1OMptk9gTHV1SsJPVhrCBMsoFRCH6mqj7fNT+VZFv3/Dbg6ELvrao9VTVTVTMb\n2DiJmiVposY5ahzgJuBgVX1kzlN3ADu75Z3AvsmXJ0n9W/JgCfBa4HeBbyc50LX9KfAh4LYku4DH\ngWv7KVGS+jXOUeN/BrLI01dMthxJGt44PUI1ZO7pMlIrvMROUvMMQknNc2j8POYwVxqPPUJJzTMI\nJTXPIJTUPOcI14BpzvUN+Y0zftONpsUeoaTmGYSSmufQuFF9Dj0nMZR/rj9jfu2LvdbhtcZlj1BS\n8wxCSc0zCCU1zznCNWDcua75c2XP1zmycecgW/l9aOXsEUpqnkEoqXkOjZ9Hpjn0W43fdONQWOOy\nRyipeQahpOY5NNayTWs47JBXk2aPUFLzDEJJzTMIJTXPOcKercbTSmDtzbOttXqfS9/7xPPpdzWU\nJXuESTYl+XqSbyZ5MMkHu/YLktyT5FCSW5Oc0X+5kjR54wyNjwOXV9XFwA7gqiSXAR8GPlpVLwee\nBnb1V6Yk9WfJoXFVFfCf3eqG7qeAy4Hf6dr3An8BfGLyJa5tq/Vqj9U6ZF/MWqt3rvn7gEPX1Wes\ngyVJ1iU5ABwF7gK+BxyrqpPdSw4D5/ZToiT1a6wgrKqfVdUOYDtwKfDKcTeQZHeS2SSzJzi+zDIl\nqT+ndfpMVR0D7gZeA5yV5NTQejtwZJH37Kmqmaqa2cDGFRUrSX1Yco4wyTnAiao6luQFwJWMDpTc\nDbwTuAXYCezrs1Cdvr7nooact5v7d/ELVzVp45xHuA3Ym2Qdox7kbVV1Z5KHgFuS/BVwP3BTj3VK\nUm/GOWr8LeCSBdofZTRfKElrmleWaM1xKKxJ81pjSc0zCCU1z6Gx1rzFjl47hNa47BFKap5BKKl5\nBqGk5jlHqFXJ+T0NyR6hpOYZhJKa59BYa95zfSGDNA57hJKaZxBKap5BKKl5zhFq2SY9NzeJU2Y8\n7UbLYY9QUvMMQknNc2isqXIoq9XAHqGk5hmEkprn0FgTMe4RZIfCWo3sEUpqnkEoqXkGoaTmOUeo\nQc2fP3TOUKvB2D3CJOuS3J/kzm79giT3JDmU5NYkZ/RXpiT153SGxtcDB+esfxj4aFW9HHga2DXJ\nwiRpKGMFYZLtwG8Dn+zWA1wO3N69ZC9wTR8FSlLfxu0Rfgx4P/Dzbv0lwLGqOtmtHwbOnXBtkjSI\nJYMwyduBo1V133I2kGR3ktkksyc4vpw/QpJ6Nc5R49cC70jyNmAT8CLg48BZSdZ3vcLtwJGF3lxV\ne4A9AC/KlppI1ZI0QUsGYVXdCNwIkOQNwJ9U1buTfBZ4J3ALsBPY12OdWoW+/G8HFmx/y8t2PGt9\n7ikzni6j1WglJ1R/APjjJIcYzRneNJmSJGlYp3VCdVV9Dfhat/wocOnkS5KkYXlliXrncFirndca\nS2qeQSipeWtuaDz3SOX8o5OaHj8LrWX2CCU1zyCU1DyDUFLz1sQc4WJXMGj1cO5Wa5k9QknNMwgl\nNW9NDI0XM3/I7JBM0nLYI5TUPINQUvMMQknNMwglNc8glNQ8g1BS89b06TNaPTx1SWuZPUJJzTMI\nJTVvTQyN5w67/AIGSZNmj1BS8wxCSc0zCCU1b03MES7GUzYkTcJYQZjkMeAZ4GfAyaqaSbIFuBU4\nH3gMuLaqnu6nTEnqz+kMjd9YVTuqaqZbvwHYX1UXAvu7dUlac1YyR3g1sLdb3gtcs/JyJGl44wZh\nAV9Jcl+S3V3b1qp6olt+Etg68eokaQDjHix5XVUdSfIrwF1JvjP3yaqqJLXQG7vg3A2wiTNXVKwk\n9WGsHmFVHekejwJfAC4FnkqyDaB7PLrIe/dU1UxVzWxg42SqlqQJWrJHmGQz8EtV9Uy3/GbgL4E7\ngJ3Ah7rHfX0WqtXH05f0fDHO0Hgr8IUkp17/91X1pST3Arcl2QU8DlzbX5mS1J8lg7CqHgUuXqD9\nJ8AVfRQlSUNac1eWOByTNGleayypeQahpOYZhJKaZxBKap5BKKl5BqGk5hmEkppnEEpqnkEoqXkG\noaTmGYSSmmcQSmqeQSipeQahpOYZhJKaZxBKap5BKKl5BqGk5hmEkppnEEpqnkEoqXkGoaTmGYSS\nmmcQSmreWEGY5Kwktyf5TpKDSV6TZEuSu5I80j2e3XexktSHcXuEHwe+VFWvBC4GDgI3APur6kJg\nf7cuSWvOkkGY5MXA64GbAKrqf6rqGHA1sLd72V7gmr6KlKQ+jdMjvAD4EfC3Se5P8skkm4GtVfVE\n95onga19FSlJfRonCNcDrwI+UVWXAP/FvGFwVRVQC705ye4ks0lmT3B8pfVK0sSNE4SHgcNVdU+3\nfjujYHwqyTaA7vHoQm+uqj1VNVNVMxvYOImaJWmilgzCqnoS+GGSV3RNVwAPAXcAO7u2ncC+XiqU\npJ6tH/N1fwB8JskZwKPA7zEK0duS7AIeB67tp0RJ6tdYQVhVB4CZBZ66YrLlSNLwvLJEUvMMQknN\nMwglNc8glNQ8g1BS8wxCSc0zCCU1L6PLhAfaWPIjRidfvxT48WAbXthqqAGsYz7reDbreLaV1vFr\nVXXO/MZBg/AXG01mq2qhE7SbqsE6rMM6VkcdDo0lNc8glNS8aQXhniltd67VUANYx3zW8WzW8Wy9\n1DGVOUJJWk0cGktq3qBBmOSqJA8nOZRksLveJflUkqNJHpjTNvjtSJOcl+TuJA8leTDJ9dOoJcmm\nJF9P8s2ujg927Rckuaf7fG7tvn+yd0nWdffDuXNadSR5LMm3kxxIMtu1TWMfmfqtc5O8ovs9nPr5\naZL3Tun38UfdPvpAkpu7fXfi+8dgQZhkHfA3wFuBi4Drklw00OY/DVw1r20atyM9Cbyvqi4CLgPe\n0/0Ohq7lOHB5VV0M7ACuSnIZ8GHgo1X1cuBpYFfPdZxyPaNbxJ4yrTreWFU75pyeMY19ZOq3zq2q\nh7vfww7gt4D/Br4wdB1JzgX+EJipqt8A1gHvoo/9o6oG+QFeA3x5zvqNwI0Dbv984IE56w8D27rl\nbcDDQ9Uyp4Z9wJXTrAU4E/gG8GpGJ6quX+jz6nH72xn9o7ocuBPIlOp4DHjpvLZBPxfgxcD36ebu\np1XHvG2/GfiXKf0+zgV+CGxh9CXSdwJv6WP/GHJofOovdcrhrm1apno70iTnA5cA90yjlm44eoDR\nTbfuAr4HHKuqk91Lhvp8Pga8H/h5t/6SKdVRwFeS3Jdkd9c29OeyGm+d+y7g5m550Dqq6gjw18AP\ngCeA/wDuo4f9w4MlPPftSPuQ5IXA54D3VtVPp1FLVf2sRkOf7cClwCv73uZ8Sd4OHK2q+4be9gJe\nV1WvYjR1854kr5/75ECfy4punTtp3dzbO4DPzn9uiDq6OcirGf0H8TJgM/9/imsihgzCI8B5c9a3\nd23TMtbtSCctyQZGIfiZqvr8NGsBqKpjwN2MhhhnJTl1H5shPp/XAu9I8hhwC6Ph8cenUMep3gdV\ndZTRfNilDP+5rOjWuT14K/CNqnqqWx+6jjcB36+qH1XVCeDzjPaZie8fQwbhvcCF3RGfMxh1ue8Y\ncPvzDX470iQBbgIOVtVHplVLknOSnNUtv4DRPOVBRoH4zqHqqKobq2p7VZ3PaH/4x6p699B1JNmc\n5JdPLTOaF3uAgT+XWn23zr2O/xsWM4U6fgBcluTM7t/Oqd/H5PePoSZdu4nNtwHfZTQf9WcDbvdm\nRnMMJxj9r7uL0VzUfuAR4KvAlgHqeB2j4cS3gAPdz9uGrgX4TeD+ro4HgD/v2n8d+DpwiNFwaOOA\nn9EbgDunUUe3vW92Pw+e2jentI/sAGa7z+YfgLOnVMdm4CfAi+e0TaOODwLf6fbTvwM29rF/eGWJ\npOZ5sERS8wxCSc0zCCU1zyCU1DyDUFLzDEJJzTMIJTXPIJTUvP8FX1BfLkvuvnoAAAAASUVORK5C\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_2pTfKw54op",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NETWORK\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 64, 3)\n",
        "        self.conv2 = nn.Conv2d(64, 64, 3)\n",
        "\n",
        "        self.maxpool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3)\n",
        "        self.conv4 = nn.Conv2d(128, 128, 3)\n",
        "\n",
        "        self.conv5 = nn.Conv2d(128, 256, 3)\n",
        "        self.conv6 = nn.Conv2d(256, 256, 3)\n",
        "\n",
        "        self.conv7 = nn.Conv2d(256, 512, 3)\n",
        "        self.conv8 = nn.Conv2d(512, 512, 3)\n",
        "\n",
        "        self.conv9 = nn.Conv2d(512, 1024, 3)\n",
        "        self.conv10 = nn.Conv2d(1024, 1024, 3)\n",
        "\n",
        "        self.upconv1 = nn.ConvTranspose2d(1024, 512, 2)\n",
        "        # cat\n",
        "        self.conv11 = nn.Conv2d(1024, 512, 3)\n",
        "        self.conv12 = nn.Conv2d(512, 512, 3)\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(512, 256, 2)\n",
        "        # cat\n",
        "        self.conv13 = nn.Conv2d(512, 256, 3)\n",
        "        self.conv14 = nn.Conv2d(256, 256, 3)\n",
        "\n",
        "        self.upconv3 = nn.ConvTranspose2d(256, 128, 2)\n",
        "        # cat\n",
        "        self.conv15 = nn.Conv2d(256, 128, 3)\n",
        "        self.conv16 = nn.Conv2d(128, 128, 3)\n",
        "\n",
        "        self.upconv4 = nn.ConvTranspose2d(128, 64, 2)\n",
        "        # cat\n",
        "        self.conv17 = nn.Conv2d(128, 64, 3)\n",
        "        self.conv18 = nn.Conv2d(64, 64, 3)\n",
        "\n",
        "        self.conv19 = nn.Conv2d(64, 21, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        a = F.relu(self.conv1(x))\n",
        "        a = self.maxpool(F.relu(self.conv2(a)))\n",
        "\n",
        "        b = F.relu(self.conv3(a))\n",
        "        b = self.maxpool(F.relu(self.conv4(b)))\n",
        "\n",
        "        c = F.relu(self.conv5(b))\n",
        "        c = self.maxpool(F.relu(self.conv6(c)))\n",
        "\n",
        "        d = F.relu(self.conv7(c))\n",
        "        d = self.maxpool(F.relu(self.conv8(d)))\n",
        "\n",
        "        e = F.relu(self.conv9(d))\n",
        "        e = self.maxpool(F.relu(self.conv10(e)))\n",
        "\n",
        "        f = self.upconv1(e)\n",
        "        f = torch.cat((d,f),dim=1)\n",
        "        f = F.relu(self.conv11(f))\n",
        "        f = F.relu(self.conv12(f))\n",
        "\n",
        "        g = self.upconv1(f)\n",
        "        g = torch.cat((c,g),dim=1)\n",
        "        g = F.relu(self.conv13(g))\n",
        "        g = F.relu(self.conv14(g))\n",
        "\n",
        "        h = self.upconv1(g)\n",
        "        h = torch.cat((b,h),dim=1)\n",
        "        h = F.relu(self.conv15(h))\n",
        "        h = F.relu(self.conv16(h))\n",
        "\n",
        "        i = self.upconv1(h)\n",
        "        i = torch.cat((a,i),dim=1)\n",
        "        i = F.relu(self.conv17(i))\n",
        "        i = F.relu(self.conv18(i))\n",
        "\n",
        "        j = self.conv1(i)\n",
        "        \n",
        "        return j\n",
        "\n",
        "\n",
        "net = Net()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLqDLY7o54o1",
        "colab_type": "text"
      },
      "source": [
        "## 8.2\n",
        "Once you have done that, we want you to redesign a network where you remove to reinjection link (grey arrow on the drawing). You can remove the both from your choice just try and tell us if it's still working and why."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDG04rye54o4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9yesinb54pF",
        "colab_type": "text"
      },
      "source": [
        "## 8.3 BONUSTOCOME"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2H22yFC54pI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
