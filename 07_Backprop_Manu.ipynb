{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 7: Backpropagation\n",
    "Read the Rojas book (https://page.mi.fu-berlin.de/rojas/neural/neuron.pdf), chapter 7.3.3 and learn about the \"matrix way\" of implementing backprop. \n",
    "\n",
    "## Ex. 7.1 XOR\n",
    "Implement a two-layer artificial neural network with two input neurons and one output neuron. Choose the number of hidden neurons to your liking and add an error \"neuron\" to your network. Our goal is to learn the XOR function. What does the network return for random weights of all combinations of (binary) inputs? **(RESULT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-07T16:17:54.708658Z",
     "start_time": "2019-12-07T16:17:54.699795Z"
    }
   },
   "outputs": [],
   "source": [
    "# WTF is an ERROR NEURON? :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-07T16:17:54.931857Z",
     "start_time": "2019-12-07T16:17:54.920162Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Helper functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def add_ones(x):\n",
    "    if len(X.shape) == 1:\n",
    "        return np.c_[[1], [X]][0]\n",
    "    else:\n",
    "        return np.c_[np.ones(len(X)), X]\n",
    "\n",
    "def without_ones(x):\n",
    "    return X[1:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-07T16:28:12.505542Z",
     "start_time": "2019-12-07T16:28:12.485588Z"
    }
   },
   "outputs": [],
   "source": [
    "class Network():\n",
    "    def __init__(self, hidden=2, m=2):\n",
    "        self.hidden = hidden\n",
    "        self.m = m\n",
    "        \n",
    "    # Exercise 7.2\n",
    "    def fit_with_backprop(self, x, y, epochs=5):\n",
    "        input_dim = len(x[0])\n",
    "        if len(np.unique(y)) == 2:\n",
    "            self.output_dim = 1\n",
    "        \n",
    "        self.init_weights(input_dim, self.output_dim, self.m)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            for x_i, y_i in zip(x,y):\n",
    "                outputs = self.feed_forward(x_i)\n",
    "        \n",
    "        print(\"TODO\")\n",
    "    \n",
    "    # Exercise 7.1\n",
    "    def fit_wo_backprop(self, x, y, epochs=1):\n",
    "        input_dim = len(x[0])\n",
    "        if len(np.unique(y)) == 2:\n",
    "            self.output_dim = 1\n",
    "        \n",
    "        self.init_weights(input_dim, self.output_dim, self.m)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            for x_i, y_i in zip(x,y):\n",
    "                #outputs = self.feed_forward(x_i)\n",
    "                score = self.predict(x_i)\n",
    "                print(\"Input: \" + str(x_i) + \" , Label: \" + str(y_i) + \" , Prediction: \" + str(score))\n",
    "        \n",
    "    def init_weights(self, input_dim, output_dim, m):\n",
    "        self.W = []\n",
    "        prev_dim = input_dim\n",
    "        hidden_layer_dims = [m, output_dim] \n",
    "        \n",
    "        for layer in range(self.hidden):\n",
    "            layer_dim = hidden_layer_dims[layer]\n",
    "            self.W.append(self.init_layer(prev_dim, layer_dim))\n",
    "            prev_dim = layer_dim\n",
    "            \n",
    "        #print(self.W[0].shape)\n",
    "        #print(self.W[1].shape)\n",
    "        #print(self.W[0])\n",
    "        #print(self.W[1])\n",
    "    \n",
    "    def init_layer(self, input_dim, output_dim):\n",
    "        return np.random.random((input_dim, output_dim))\n",
    "    \n",
    "    def feed_forward(self, x):\n",
    "        outputs = [x]\n",
    "        out_last = x\n",
    "        \n",
    "        for W_i in self.W:\n",
    "            #out_hat = add_ones(out_last)\n",
    "            #out_last = sigmoid(out_hat.dot(Wi))\n",
    "            out_last = sigmoid(out_last.dot(W_i))\n",
    "            outputs.append(out_last)\n",
    "                \n",
    "        return outputs\n",
    "    \n",
    "    def backprop():\n",
    "        pass\n",
    "    \n",
    "    def predict(self, x):\n",
    "        out_last = self.feed_forward(x)[-1]\n",
    "        out_last[out_last > .5] = 1\n",
    "        out_last[out_last <= .5] = 0\n",
    "        return out_last\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-07T16:28:13.057256Z",
     "start_time": "2019-12-07T16:28:13.048292Z"
    }
   },
   "outputs": [],
   "source": [
    "# Datasets\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "\n",
    "y_and = [a & b for a, b in X]\n",
    "y_or = [a | b for a, b in X]\n",
    "y_xor = [a ^ b for a, b in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-07T16:28:13.664615Z",
     "start_time": "2019-12-07T16:28:13.630469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################\n",
      "####  Exercise 7.1  ####\n",
      "########################\n",
      "Input: [0 0] , Label: 0 , Prediction: [ 1.]\n",
      "Input: [0 1] , Label: 1 , Prediction: [ 1.]\n",
      "Input: [1 0] , Label: 1 , Prediction: [ 1.]\n",
      "Input: [1 1] , Label: 0 , Prediction: [ 1.]\n",
      "########################\n",
      "####  Exercise 7.2  ####\n",
      "########################\n",
      "TODO\n"
     ]
    }
   ],
   "source": [
    "'param1 : number of hidden layers, param2: m -> number of nodes per hidden layer'\n",
    "net = Network(2, 2)\n",
    "\n",
    "print(\"########################\")\n",
    "print(\"####  Exercise 7.1  ####\")\n",
    "print(\"########################\")\n",
    "net.fit_wo_backprop(X, y_xor)\n",
    "\n",
    "print(\"########################\")\n",
    "print(\"####  Exercise 7.2  ####\")\n",
    "print(\"########################\")\n",
    "net.fit_with_backprop(X, y_xor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex. 7.2 Backpropagation\n",
    "Implement Backpropagation and optimize the weights of your neural network using the XOR training set: \n",
    "\n",
    "#### x, y\n",
    "\n",
    "(0,0), 0 \n",
    "\n",
    "(0,1), 1\n",
    "\n",
    "(1,0), 1\n",
    "\n",
    "(1,1), 0\n",
    "\n",
    "How many training iterations do you need? Plot the network error over the number of iterations! **(RESULT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 7.3 MNIST (BONUS)\n",
    "Train your network on the [MNIST dataset](http://yann.lecun.com/exdb/mnist/) and state the model accuracy (or the model error) for the training and test sets. **(RESULT)** Compare to this [list](https://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#4d4e495354)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
