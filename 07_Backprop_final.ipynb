{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 7: Backpropagation\n",
    "Read the Rojas book (https://page.mi.fu-berlin.de/rojas/neural/neuron.pdf), chapter 7.3.3 and learn about the \"matrix way\" of implementing backprop. \n",
    "\n",
    "## Ex. 7.1 XOR\n",
    "Implement a two-layer artificial neural network with two input neurons and one output neuron. Choose the number of hidden neurons to your liking and add an error \"neuron\" to your network. Our goal is to learn the XOR function. What does the network return for random weights of all combinations of (binary) inputs? **(RESULT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T18:48:45.429944Z",
     "start_time": "2019-12-18T18:48:45.422071Z"
    }
   },
   "outputs": [],
   "source": [
    "# WTF is an ERROR NEURON? :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T18:48:46.110898Z",
     "start_time": "2019-12-18T18:48:45.787641Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Helper functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    v = sigmoid(x)\n",
    "    return v * (1 - v)\n",
    "\n",
    "def relu(x):\n",
    "    x[x <= 0.0] = 0.0\n",
    "    return x\n",
    "\n",
    "def augmented(array):\n",
    "    \"\"\"Add ones to 0-axis.\"\"\"\n",
    "    shape = array.shape\n",
    "    ones = np.ones((1, *shape[1:]))\n",
    "    items = (array, ones)\n",
    "    return np.concatenate(items, axis=0)\n",
    "\n",
    "\n",
    "def unaugmented(array):\n",
    "    \"\"\"Inverse operation to 'augmented'.\"\"\"\n",
    "    return array[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T18:48:47.426693Z",
     "start_time": "2019-12-18T18:48:47.417585Z"
    }
   },
   "outputs": [],
   "source": [
    "# Datasets\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "\n",
    "y_and = [a & b for a, b in X]\n",
    "y_or = [a | b for a, b in X]\n",
    "y_xor = [a ^ b for a, b in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T18:48:47.987026Z",
     "start_time": "2019-12-18T18:48:47.977499Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class GradientDescent:\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def __call__(self, weights: List[np.array], gradients: List) -> List:\n",
    "        return [\n",
    "            weight - self.lr * gradient\n",
    "            for weight, gradient in zip(weights, gradients)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T18:48:48.833664Z",
     "start_time": "2019-12-18T18:48:48.809295Z"
    }
   },
   "outputs": [],
   "source": [
    "class Network():\n",
    "    W = None\n",
    "    biases = None\n",
    "    \n",
    "    def __init__(self, hidden, m, optimizer=GradientDescent(5.), output_dim=None):\n",
    "        \"\"\"\n",
    "        :param hidden: number of hidden layers\n",
    "        :param m: number of nodes per hidden layer\n",
    "        \"\"\"\n",
    "        self.hidden = hidden\n",
    "        self.m = m\n",
    "        self.optimizer = optimizer\n",
    "        self.output_dim = output_dim\n",
    "    \n",
    "    @staticmethod\n",
    "    def init_layer(input_dim, output_dim):\n",
    "        shape = (input_dim, output_dim)\n",
    "        # return np.random.uniform(0, 1, shape)\n",
    "        # return np.random.random(shape)\n",
    "        # return np.random.random(shape) - 0.5\n",
    "        return np.random.random(shape) * np.sqrt(2.0 / (input_dim + output_dim))\n",
    "    \n",
    "    def initialize(self, x, y):\n",
    "        if self.output_dim is None:\n",
    "            self.output_dim = len(np.unique(y))\n",
    "            \n",
    "        input_dim = len(x[0])        \n",
    "        self.W = self.init_weights(input_dim, self.output_dim, self.m)\n",
    "        # The number of biases equals each weight matrix'es output_dim.\n",
    "        self.biases = [np.ones(W_i.shape[1]) for W_i in self.W]\n",
    "    \n",
    "    def init_weights(self, input_dim, output_dim, m):\n",
    "        W = []\n",
    "        prev_dim = input_dim\n",
    "        layer_dims = ([m] * self.hidden) + [output_dim]\n",
    "        \n",
    "        for layer_dim in layer_dims:\n",
    "            W.append(self.init_layer(prev_dim, layer_dim))\n",
    "            prev_dim = layer_dim\n",
    "        return W\n",
    "    \n",
    "    def feed_forward(self, x):\n",
    "        \"\"\"\n",
    "        DIFFERENT WAY OF ADDING BIASES:\n",
    "        (aug = augmentation function)\n",
    "        \n",
    "        Rojas' book:\n",
    "        O.T * W = y.T\n",
    "        \n",
    "        O1  ~> aug(O1) * aug(W_1) = y1 ~> s(y1) = O2\n",
    "        40x1    41x1      41x50    50x1   50x1   50x1\n",
    "        \n",
    "        \n",
    "        This is equal to\n",
    "        O1  ~>  O1 * W_1  =  y1 + bias ~> s(y1) = O2\n",
    "        40x    40x1 40x50   50x1  50x1    50x1\n",
    "        because due to the vector-matrix multiplication \n",
    "        each entry of the last row of W_1 \n",
    "        is added to each value of y1:\n",
    "        y1 = [\n",
    "            O1[0]*W_1[0,0] + O1[1]*W_1[1,0] + ... + O1[n]*W_1[n,0] ( + 1*W_1[n+1,0] )\n",
    "            ...\n",
    "            O1[m]*W_1[0,m] + O1[1]*W_1[1,m] + ... + O1[n]*W_1[n,m] ( + 1*W_1[n+1,m] )\n",
    "                                                                            |\n",
    "                                                                 This is where the bias\n",
    "                                                                 would be added when\n",
    "                                                                 doing it like Rojas.\n",
    "        ]\n",
    "        As we can see adding the bias as an extra vector does the same thing.\n",
    "        \"\"\"\n",
    "        \n",
    "        out_last = x\n",
    "        outputs = [out_last]\n",
    "        \n",
    "        for W_i, bias in zip(self.W, self.biases):\n",
    "            out_last = sigmoid((out_last @ W_i) + bias)\n",
    "            outputs.append(out_last)\n",
    "        return outputs\n",
    "    \n",
    "    def backprop(self, outputs, y_i) -> List:\n",
    "        \"\"\"\n",
    "        HOW TO UPDATE BIASES:\n",
    "        (aug = augmentation function)\n",
    "        \n",
    "        Rojas suggests: LR * delta * aug(O)\n",
    "        \n",
    "        Thus, the new bias for a given layer is the entry of the augmented\n",
    "        output vector multiplied with 'delta'. Since the last entry is always 1\n",
    "        'delta' itself describes the change of the biases. For that reason,\n",
    "        they are appended and returned by this method as well.\n",
    "        \"\"\"\n",
    "        gradients = []\n",
    "        bias_gradients = []\n",
    "        out_last = outputs[-1]\n",
    "        out_last_prev = outputs[-2]\n",
    "        \n",
    "        e = out_last - y_i\n",
    "        \n",
    "        D = np.diag(sigmoid_prime(out_last))\n",
    "        delta = D.dot(e)\n",
    "        \n",
    "        gradient = np.outer(delta, out_last_prev).T\n",
    "        gradients.append(gradient)\n",
    "        bias_gradients.append(delta)\n",
    "\n",
    "        for i in reversed(range(self.hidden)):\n",
    "            output_idx = i + 1\n",
    "            output = outputs[output_idx]\n",
    "            D = np.diag(sigmoid_prime(output))\n",
    "            delta = D.dot(self.W[output_idx]).dot(delta)\n",
    "            gradients.append(np.outer(delta, outputs[output_idx - 1]).T)\n",
    "            bias_gradients.append(delta)\n",
    "        \n",
    "        gradients.reverse()\n",
    "        bias_gradients.reverse()\n",
    "        return gradients, bias_gradients\n",
    "\n",
    "    def step(self, x_i, y_i):\n",
    "        \"\"\"\n",
    "        :parma y_i: The value for y_i itself \n",
    "                    or a function the retrieve the value from the outputs.\n",
    "                    The latter is used for one-hot encoding.\n",
    "        \"\"\"\n",
    "        outputs = self.feed_forward(x_i)\n",
    "        if callable(y_i):\n",
    "            y_i = y_i(outputs)\n",
    "        gradients, bias_gradients = self.backprop(outputs, y_i)\n",
    "        self.W = self.optimizer(self.W, gradients)\n",
    "        self.biases = self.optimizer(self.biases, bias_gradients)\n",
    "        return outputs, gradients, bias_gradients\n",
    "    \n",
    "    def predict(self, x):\n",
    "        out_last = self.feed_forward(x)[-1]\n",
    "        out_last[out_last > .5] = 1\n",
    "        out_last[out_last <= .5] = 0\n",
    "        return out_last\n",
    "    \n",
    "    def predict_onehot(self, x):\n",
    "        out_last = self.feed_forward(x)[-1]\n",
    "        out_last = np.array(np.argmax(out_last) + 1)\n",
    "        out_last = out_last.astype(np.float64)\n",
    "        return out_last\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T18:48:49.877969Z",
     "start_time": "2019-12-18T18:48:49.843922Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####  Exercise 7.1  ####\n",
      "Input: [0 0] , Label: 0 , Prediction: [1.]\n",
      "Input: [0 1] , Label: 1 , Prediction: [1.]\n",
      "Input: [1 0] , Label: 1 , Prediction: [1.]\n",
      "Input: [1 1] , Label: 0 , Prediction: [1.]\n"
     ]
    }
   ],
   "source": [
    "class NetXor(Network):\n",
    "    def fit_wo_backprop(self, x, y, epochs=1):\n",
    "        self.initialize(x, y)\n",
    "        for epoch in range(epochs):\n",
    "            for x_i, y_i in zip(x, y):\n",
    "                prediction = self.predict(x_i)\n",
    "                print(\"Input: \" + str(x_i) + \" , Label: \" + str(y_i) + \" , Prediction: \" + str(prediction))\n",
    "\n",
    "\n",
    "'param1 : number of hidden layers, param2: m -> number of nodes per hidden layer'\n",
    "net = NetXor(2, 2, GradientDescent(.55), output_dim=1)\n",
    "\n",
    "print(\"####  Exercise 7.1  ####\")\n",
    "net.fit_wo_backprop(X, y_xor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex. 7.2 Backpropagation\n",
    "Implement Backpropagation and optimize the weights of your neural network using the XOR training set: \n",
    "\n",
    "#### x, y\n",
    "\n",
    "(0,0), 0 \n",
    "\n",
    "(0,1), 1\n",
    "\n",
    "(1,0), 1\n",
    "\n",
    "(1,1), 0\n",
    "\n",
    "How many training iterations do you need? Plot the network error over the number of iterations! **(RESULT)**\n",
    "\n",
    "**We needed a few iterations (1 or 2) with a standard gradient descent learning rate of 3.0**\n",
    "\n",
    "**Error = 1 - Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T19:06:01.884266Z",
     "start_time": "2019-12-18T19:06:01.795066Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####  Exercise 7.2  ####\n",
      "Epoch: 0\n",
      "Input: [0 0], Label: 0, Prediction: [1.]\n",
      "Input: [0 1], Label: 1, Prediction: [1.]\n",
      "Input: [1 0], Label: 1, Prediction: [1.]\n",
      "Input: [1 1], Label: 0, Prediction: [1.]\n",
      "predicttrue: 2; predictfalse: 2\n",
      "Accuracy after epoch 0: 0.5\n",
      "Epoch: 1\n",
      "Input: [0 0], Label: 0, Prediction: [0.]\n",
      "Input: [0 1], Label: 1, Prediction: [1.]\n",
      "Input: [1 0], Label: 1, Prediction: [1.]\n",
      "Input: [1 1], Label: 0, Prediction: [0.]\n",
      "predicttrue: 4; predictfalse: 0\n",
      "Accuracy after epoch 1: 1.0\n",
      "Epoch: 2\n",
      "Input: [0 0], Label: 0, Prediction: [0.]\n",
      "Input: [0 1], Label: 1, Prediction: [1.]\n",
      "Input: [1 0], Label: 1, Prediction: [1.]\n",
      "Input: [1 1], Label: 0, Prediction: [0.]\n",
      "predicttrue: 4; predictfalse: 0\n",
      "Accuracy after epoch 2: 1.0\n",
      "Epoch: 3\n",
      "Input: [0 0], Label: 0, Prediction: [0.]\n",
      "Input: [0 1], Label: 1, Prediction: [1.]\n",
      "Input: [1 0], Label: 1, Prediction: [1.]\n",
      "Input: [1 1], Label: 0, Prediction: [0.]\n",
      "predicttrue: 4; predictfalse: 0\n",
      "Accuracy after epoch 3: 1.0\n",
      "Epoch: 4\n",
      "Input: [0 0], Label: 0, Prediction: [0.]\n",
      "Input: [0 1], Label: 1, Prediction: [1.]\n",
      "Input: [1 0], Label: 1, Prediction: [1.]\n",
      "Input: [1 1], Label: 0, Prediction: [0.]\n",
      "predicttrue: 4; predictfalse: 0\n",
      "Accuracy after epoch 4: 1.0\n",
      "Epoch: 5\n",
      "Input: [0 0], Label: 0, Prediction: [0.]\n",
      "Input: [0 1], Label: 1, Prediction: [1.]\n",
      "Input: [1 0], Label: 1, Prediction: [1.]\n",
      "Input: [1 1], Label: 0, Prediction: [0.]\n",
      "predicttrue: 4; predictfalse: 0\n",
      "Accuracy after epoch 5: 1.0\n",
      "Epoch: 6\n",
      "Input: [0 0], Label: 0, Prediction: [0.]\n",
      "Input: [0 1], Label: 1, Prediction: [1.]\n",
      "Input: [1 0], Label: 1, Prediction: [1.]\n",
      "Input: [1 1], Label: 0, Prediction: [0.]\n",
      "predicttrue: 4; predictfalse: 0\n",
      "Accuracy after epoch 6: 1.0\n",
      "Epoch: 7\n",
      "Input: [0 0], Label: 0, Prediction: [0.]\n",
      "Input: [0 1], Label: 1, Prediction: [1.]\n",
      "Input: [1 0], Label: 1, Prediction: [1.]\n",
      "Input: [1 1], Label: 0, Prediction: [0.]\n",
      "predicttrue: 4; predictfalse: 0\n",
      "Accuracy after epoch 7: 1.0\n",
      "Epoch: 8\n",
      "Input: [0 0], Label: 0, Prediction: [0.]\n",
      "Input: [0 1], Label: 1, Prediction: [1.]\n",
      "Input: [1 0], Label: 1, Prediction: [1.]\n",
      "Input: [1 1], Label: 0, Prediction: [0.]\n",
      "predicttrue: 4; predictfalse: 0\n",
      "Accuracy after epoch 8: 1.0\n",
      "Epoch: 9\n",
      "Input: [0 0], Label: 0, Prediction: [0.]\n",
      "Input: [0 1], Label: 1, Prediction: [1.]\n",
      "Input: [1 0], Label: 1, Prediction: [1.]\n",
      "Input: [1 1], Label: 0, Prediction: [0.]\n",
      "predicttrue: 4; predictfalse: 0\n",
      "Accuracy after epoch 9: 1.0\n"
     ]
    }
   ],
   "source": [
    "class NetBackProp(Network):\n",
    "    'TODO: fix to old version'\n",
    "    def fit_with_backprop(self, x, y, epochs):\n",
    "        self.initialize(x, y)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print(\"Epoch: \" + str(epoch))\n",
    "            predict_true = 0\n",
    "            predict_false = 0\n",
    "            \n",
    "            for x_i, y_i in zip(x, y):\n",
    "                self.step(x_i, y_i)\n",
    "                prediction = self.predict(x_i)\n",
    "                \n",
    "                if prediction == y_i:\n",
    "                    predict_true += 1\n",
    "                else:\n",
    "                    predict_false += 1\n",
    "                    \n",
    "                print(\"Input: \" + str(x_i) + \", Label: \" + str(y_i) + \", Prediction: \" + str(prediction))\n",
    "                \n",
    "            accuracy = predict_true / (predict_true + predict_false)   \n",
    "            print(\"predicttrue: \" + str(predict_true) + \"; predictfalse: \" + str(predict_false))\n",
    "            print(\"Accuracy after epoch {}: {}\".format(epoch, accuracy))\n",
    "\n",
    "\n",
    "print(\"####  Exercise 7.2  ####\")\n",
    "net = NetBackProp(2, 2, GradientDescent(3.), output_dim=1)\n",
    "net.fit_with_backprop(X, y_xor, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 7.3 MNIST (BONUS)\n",
    "Train your network on the [MNIST dataset](http://yann.lecun.com/exdb/mnist/) and state the model accuracy (or the model error) for the training and test sets. **(RESULT)** Compare to this [list](https://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#4d4e495354)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T19:06:19.726244Z",
     "start_time": "2019-12-18T19:06:14.555776Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "4.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "mnist_data = torchvision.datasets.MNIST('./MNIST', train=True, transform=None, target_transform=None, download=True)\n",
    "#data_loader = torch.utils.data.DataLoader(mnist_data, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "# Get data as numpy\n",
    "np_images = np.empty([len(mnist_data), 28, 28])\n",
    "np_labels = np.empty([len(mnist_data)])\n",
    "\n",
    "for i, (image, label) in enumerate(mnist_data):\n",
    "    data = (image, label)\n",
    "    np_images[i] = np.array(image)\n",
    "    np_labels[i] = label\n",
    "    \n",
    "print(np_images.shape)\n",
    "print(np_labels.shape)\n",
    "\n",
    "#Flatten images first\n",
    "images_flat = np_images.reshape(-1, 28*28)\n",
    "\n",
    "#Normalize\n",
    "images_flat = images_flat[:] / 255\n",
    "\n",
    "# Show one example\n",
    "print(np_labels[9])\n",
    "plt.imshow(np_images[9])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T19:08:06.163068Z",
     "start_time": "2019-12-18T19:06:36.498518Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####  Exercise 7.3  ####\n",
      "Epoch: 0\n",
      "predicttrue: 6265; predictfalse: 53735\n",
      "Accuracy after epoch 0: 0.10441666666666667\n",
      "Epoch: 1\n",
      "predicttrue: 6265; predictfalse: 53735\n",
      "Accuracy after epoch 1: 0.10441666666666667\n",
      "Epoch: 2\n",
      "predicttrue: 6265; predictfalse: 53735\n",
      "Accuracy after epoch 2: 0.10441666666666667\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def one_hot_encode(outputs):\n",
    "    max_value = np.argmax(outputs[-1])\n",
    "    y_i_oh = np.zeros((10,))\n",
    "    y_i_oh[max_value] = 1.0\n",
    "    return y_i_oh\n",
    "\n",
    "\n",
    "class NetMnist(Network):\n",
    "    def fit_mnist(self, x, y, epochs):\n",
    "        self.initialize(x, y)\n",
    "        N = len(x)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            i = 0\n",
    "            print(\"Epoch: \" + str(epoch))\n",
    "            predict_true = 0\n",
    "            predict_false = 0\n",
    "            \n",
    "            for x_i, y_i in zip(x, y):\n",
    "                outputs, gradients, bias_gradients = self.step(x_i, y_i=one_hot_encode)\n",
    "                prediction = self.predict_onehot(x_i)\n",
    "                \n",
    "                if prediction == y_i:\n",
    "                    predict_true += 1\n",
    "                else:\n",
    "                    predict_false += 1\n",
    "\n",
    "                i += 1\n",
    "            \n",
    "            accuracy = predict_true / N   \n",
    "            print(\"predicttrue: \" + str(predict_true) + \"; predictfalse: \" + str(predict_false))\n",
    "            print(\"Accuracy after epoch {}: {}\".format(epoch, accuracy))\n",
    "\n",
    "print(\"####  Exercise 7.3  ####\")\n",
    "net_mnist = NetMnist(hidden=2, m=3, optimizer=GradientDescent(3.), output_dim=10)\n",
    "net_mnist.fit_mnist(images_flat, np_labels, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T19:08:09.736338Z",
     "start_time": "2019-12-18T19:08:09.732914Z"
    }
   },
   "outputs": [],
   "source": [
    "#PROBLEM: Gradients[0] sind alle null. => Daher kein Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Solution (closer to Rojas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T19:08:12.192854Z",
     "start_time": "2019-12-18T19:08:12.123207Z"
    }
   },
   "outputs": [],
   "source": [
    "# UTILS\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import numpy.linalg\n",
    "import numpy.matlib\n",
    "\n",
    "\n",
    "def hot_one_encode_ints(num_classes, ints):\n",
    "    \"\"\"\n",
    "    'ints' May also be a single int.\n",
    "    \"\"\"\n",
    "    # See https://stackoverflow.com/a/42874726/6928824\n",
    "    targets = np.array(ints).reshape(-1)\n",
    "    one_hot_targets = np.eye(num_classes)[targets]\n",
    "    return one_hot_targets.reshape(-1)\n",
    "\n",
    "\n",
    "def hot_one_decode_int(encoded):\n",
    "    return np.argmax(encoded, axis=0)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_d(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return np.clip(x, 0, np.inf)\n",
    "\n",
    "\n",
    "def relu_d(x):\n",
    "    return (x >= 0).astype(float)\n",
    "\n",
    "\n",
    "def augmented(array, append=True):\n",
    "    \"\"\"Add ones to 0-axis.\"\"\"\n",
    "    shape = array.shape\n",
    "    ones = np.ones((1, *shape[1:]))\n",
    "    if append:\n",
    "        items = (array, ones)\n",
    "    else:\n",
    "        items = (ones, array)\n",
    "    return np.concatenate(items, axis=0)\n",
    "\n",
    "\n",
    "def unaugmented(array, appended=True):\n",
    "    \"\"\"Inverse operation to 'augmented'.\"\"\"\n",
    "    if appended:\n",
    "        s = np.s_[:-1]\n",
    "    else:\n",
    "        s = np.s_[1:]\n",
    "    return array[s]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# CLASSIFIER\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Classifier(ABC):\n",
    "    \"\"\"\n",
    "    Abstract superclass for all classifiers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, y, num_classes=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.num_classes = num_classes or len(set(y))\n",
    "\n",
    "    @classmethod\n",
    "    def trained(cls, X, y):\n",
    "        instance = cls(X, y)\n",
    "        instance.train(X, y)\n",
    "        return instance\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, X, y):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict_label(self, x_test):\n",
    "        pass\n",
    "\n",
    "    def get_confusion_matrix(self, X_test, y_test, shape=None, **kwargs):\n",
    "        if shape is None:\n",
    "            shape = (self.num_classes, self.num_classes)\n",
    "        matrix = np.zeros(shape=shape)\n",
    "        for i, x in enumerate(X_test):\n",
    "            true_label = int(y_test[i])\n",
    "            predicted_label = self.predict_label(x, **kwargs)\n",
    "            matrix[true_label][predicted_label] += 1\n",
    "        return matrix\n",
    "\n",
    "    def print_confusion_matrix(self, X_test, y_test):\n",
    "        matrix = self.get_confusion_matrix(X_test, y_test)\n",
    "        print(matrix)\n",
    "        print('accuracy: {}'.format(self.accuracy(X_test, y_test, matrix)))\n",
    "        return matrix\n",
    "\n",
    "    def accuracy(self, X_test, y_test, matrix=None, **kwargs):\n",
    "        if matrix is None:\n",
    "            matrix = self.get_confusion_matrix(X_test, y_test, **kwargs)\n",
    "        return np.sum(np.diag(matrix)) / len(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# NETWORK\n",
    "import math\n",
    "from typing import Any, Callable, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class BatchMethod:\n",
    "    BATCH = 0\n",
    "    MINI_BATCH = 1\n",
    "    ONLINE_BATCH = 2\n",
    "\n",
    "\n",
    "class NeuralNetwork(Classifier):\n",
    "    learning_constant = 1e-3\n",
    "\n",
    "    def __init__(self, X, y,\n",
    "                 size_in: int, size_out: int,\n",
    "                 hidden_layers: List[int],\n",
    "                 hot_one_encode_y: Callable[[int, Any], np.ndarray] = hot_one_encode_ints,\n",
    "                 hot_one_decode: Callable[[np.ndarray], int] = hot_one_decode_int):\n",
    "        \"\"\"\n",
    "        'size_in' Number of features.\n",
    "        'size_out' Number of classes.\n",
    "        'hidden_layers' Defines how many nodes each layer has.\n",
    "        'hot_one_encode_y' Hot-one encodes a label.\n",
    "        \"\"\"\n",
    "        super().__init__(X, y, num_classes=size_out)\n",
    "        assert len(hidden_layers) > 0, 'Need at least 1 hidden layer.'\n",
    "\n",
    "        self.size_in = size_in\n",
    "        self.size_out = size_out\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.hot_one_encode_y = hot_one_encode_y\n",
    "        self.hot_one_decode = hot_one_decode\n",
    "\n",
    "    def train(self, X, y, *,\n",
    "              num_epochs=10,\n",
    "              batch_method=BatchMethod.MINI_BATCH,\n",
    "              batch_size=None,\n",
    "              learning_constant=1e-3,\n",
    "              callback=None):\n",
    "        N = len(X)\n",
    "        if batch_method == BatchMethod.MINI_BATCH:\n",
    "            if batch_size is None:\n",
    "                batch_size = N // 20\n",
    "        elif batch_method == BatchMethod.BATCH:\n",
    "            batch_size = N\n",
    "            if batch_size is not None:\n",
    "                print('WARNING: batch_size given but ignored.')\n",
    "        elif batch_method == BatchMethod.ONLINE_BATCH:\n",
    "            batch_size = 1\n",
    "            if batch_size is not None:\n",
    "                print('WARNING: batch_size given but ignored.')\n",
    "        else:\n",
    "            raise ValueError('Invalid batch method.')\n",
    "\n",
    "        X_shuffled = X[:]\n",
    "        np.random.shuffle(X_shuffled)\n",
    "\n",
    "        weights, augmented_weights = self._initialize_weight_matrices()\n",
    "        num_batches = math.ceil(N / batch_size)\n",
    "        for epoch in range(num_epochs):\n",
    "            for batch_index in range(num_batches):\n",
    "                batch = X_shuffled[batch_index:(batch_index + batch_size)]\n",
    "                corrections = [\n",
    "                    np.zeros(matrix.shape)\n",
    "                    for matrix in augmented_weights\n",
    "                ]\n",
    "                for i, x in enumerate(batch):\n",
    "                    new_corrections = self.backpropagation(\n",
    "                        weights,\n",
    "                        *self.feed_forward(augmented_weights, x, y[i]),\n",
    "                        learning_constant=learning_constant,\n",
    "                    )\n",
    "                    corrections = self._sum_matrix_lists(\n",
    "                        corrections,\n",
    "                        new_corrections\n",
    "                    )\n",
    "\n",
    "                weights, augmented_weights = self._apply_weight_corrections(\n",
    "                    augmented_weights,\n",
    "                    corrections\n",
    "                )\n",
    "            print(f'epoch {epoch + 1} done')\n",
    "            if callable(callback):\n",
    "                callback(augmented_weights)\n",
    "\n",
    "        self.weights = augmented_weights\n",
    "\n",
    "    def feed_forward(self, weights, x, y_i):\n",
    "        outputs = [self._O_hat(x)]\n",
    "        diagonals = []\n",
    "        s = sigmoid\n",
    "        sd = sigmoid_d\n",
    "\n",
    "        # Start at 1 to match math notation.\n",
    "        for i, augmented_matrix in enumerate(weights, start=1):\n",
    "            O_hat_prev = outputs[i - 1]\n",
    "            W = augmented_matrix\n",
    "            O = s(O_hat_prev.T @ W)\n",
    "            D = np.diag(sd(O))\n",
    "\n",
    "            outputs.append(self._O_hat(O))\n",
    "            diagonals.append(D)\n",
    "        try:\n",
    "            t = self.hot_one_encode_y(self.num_classes, int(y_i))\n",
    "        except IndexError as e:\n",
    "            raise ValueError((\n",
    "                'Cannot hot one encode \"{}\" because too few outputs '\n",
    "                '(change \"size_out\" argument for \"__init__\")'\n",
    "            ).format(int(y_i))) from e\n",
    "\n",
    "        # O is the final (unaugmented) output.\n",
    "        error = O - t\n",
    "        return outputs, diagonals, error\n",
    "\n",
    "    def backpropagation(self, weights, outputs, diagonals, error,\n",
    "                        *,\n",
    "                        learning_constant):\n",
    "        \"\"\"\n",
    "        'weights' Weight matrices W_i.\n",
    "        'outputs' Augmented output vectors.\n",
    "        'diagonals' Diagonal matrices D_i containing derivates\n",
    "        'error' Error derivate vector e\n",
    "        \"\"\"\n",
    "        N = len(diagonals)\n",
    "        deltas = []\n",
    "        i_max = N - 1\n",
    "        for i in range(i_max, -1, -1):\n",
    "            D = diagonals[i]\n",
    "            if i == i_max:\n",
    "                delta = D @ error\n",
    "            else:\n",
    "                W = weights[i + 1]\n",
    "                delta = D @ W @ prev_delta\n",
    "            # Prepend delta to keep order equal to the other variables.\n",
    "            deltas.insert(0, delta)\n",
    "            prev_delta = delta\n",
    "\n",
    "        # The corrections' indices must be ascending\n",
    "        # to match the order of weight matrices.\n",
    "        return [\n",
    "            -learning_constant * np.outer(delta, outputs[i]).T\n",
    "            for i, delta in enumerate(deltas)\n",
    "        ]\n",
    "\n",
    "    def predict_label(self, x_test, weights=None):\n",
    "        \"\"\"\n",
    "        'weights' Override self.weights, used for accuracy measurement.\n",
    "        \"\"\"\n",
    "        if weights is None:\n",
    "            weights = self.weights\n",
    "\n",
    "        O_hat_prev = self._O_hat(x_test)\n",
    "        s = sigmoid\n",
    "\n",
    "        # Start at 1 to match math notation.\n",
    "        for i, augmented_matrix in enumerate(weights, start=1):\n",
    "            W = augmented_matrix\n",
    "            O = s(O_hat_prev.T @ W)\n",
    "            O_hat_prev = self._O_hat(O)\n",
    "\n",
    "        return self.hot_one_decode(O)\n",
    "\n",
    "    def _initialize_weight_matrices(self) -> Tuple[List[np.ndarray]]:\n",
    "        matrices = []\n",
    "        prev_dim_size = self.size_in\n",
    "        for layer_size in self.hidden_layers:\n",
    "            shape = (prev_dim_size, layer_size)\n",
    "            matrix = np.random.uniform(0, 1, shape)\n",
    "            matrices.append(matrix)\n",
    "            prev_dim_size = layer_size\n",
    "\n",
    "        shape = (prev_dim_size, self.size_out)\n",
    "        matrix = np.random.uniform(0, 1, shape)\n",
    "        matrices.append(matrix)\n",
    "        return matrices, [augmented(matrix) for matrix in matrices]\n",
    "\n",
    "    def _apply_weight_corrections(self, augmented_weights, corrections):\n",
    "        corrected_weights = self._sum_matrix_lists(\n",
    "            augmented_weights,\n",
    "            corrections\n",
    "        )\n",
    "        return (\n",
    "            [unaugmented(matrix) for matrix in corrected_weights],\n",
    "            corrected_weights,\n",
    "        )\n",
    "\n",
    "    def _sum_matrix_lists(self, a, b):\n",
    "        # TODO: Use zip\n",
    "        if len(a) != len(b):\n",
    "            raise ValueError('Unequally long lists.')\n",
    "        return [a_i + b[i] for i, a_i in enumerate(a)]\n",
    "\n",
    "    def _O_hat(self, O):\n",
    "        return augmented(O)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T19:10:24.769854Z",
     "start_time": "2019-12-18T19:08:15.528145Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####  Exercise 7.3b  ####\n",
      "(10000, 28, 28)\n",
      "(10000,)\n",
      "epoch 1 done\n",
      "accuracy 0.1135\n",
      "epoch 2 done\n",
      "accuracy 0.1135\n",
      "epoch 3 done\n",
      "accuracy 0.1135\n",
      "epoch 4 done\n",
      "accuracy 0.1135\n",
      "epoch 5 done\n",
      "accuracy 0.1135\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111560400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "print(\"####  Exercise 7.3b  ####\")\n",
    "\n",
    "mnist_data_test = torchvision.datasets.MNIST('./MNIST', train=False, transform=None, target_transform=None, download=True)\n",
    "# Get data as numpy\n",
    "np_images_test = np.empty([len(mnist_data_test), 28, 28])\n",
    "np_labels_test = np.empty([len(mnist_data_test)])\n",
    "\n",
    "for i, (image, label) in enumerate(mnist_data_test):\n",
    "    data = (image, label)\n",
    "    np_images_test[i] = np.array(image)\n",
    "    np_labels_test[i] = label\n",
    "    \n",
    "print(np_images_test.shape)\n",
    "print(np_labels_test.shape)\n",
    "\n",
    "#Flatten images first\n",
    "images_flat_test = np_images_test.reshape(-1, 28*28)\n",
    "\n",
    "#Normalize\n",
    "images_flat_test = images_flat_test[:] / 255\n",
    "\n",
    "\n",
    "accuracies = []\n",
    "NUM_EPOCHS=5\n",
    "\n",
    "def cb(weights):\n",
    "    acc = net_mnist_b.accuracy(images_flat_test, np_labels_test, weights=weights)\n",
    "    print('accuracy', acc)\n",
    "    accuracies.append(acc)\n",
    "\n",
    "net_mnist_b = NeuralNetwork(images_flat, np_labels, 28*28, 10, [30, 20])\n",
    "net_mnist_b.train(\n",
    "    images_flat, np_labels,\n",
    "    batch_size=32,\n",
    "    learning_constant=1e-2,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    callback=cb,\n",
    ")\n",
    "plt.plot(accuracies)\n",
    "plt.axis([0, NUM_EPOCHS, 0, 1])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignments",
   "language": "python",
   "name": "assignments"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
