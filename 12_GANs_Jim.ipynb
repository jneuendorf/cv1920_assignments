{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 12\n",
    "\n",
    "In this tutorium, we will work on GANs networks (https://arxiv.org/pdf/1406.2661.pdf). This kind of network is part of the generative networks who are actually able to generate synthetic data. This network is actually composed of two, a generator responsible for generate fake data and a discriminator for investigating on the trueness of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://pathmind.com/images/wiki/GANs.png\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://pathmind.com/images/wiki/GANs.png\", width=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The disciminator can be see as a very simple fully connected network, composed of several layer of neurons and a simple output of one neurone who just determine if a image is true or false.\n",
    "\n",
    "The generator looks more like the upscaling part of the segmenetation network. A small piece of noise is injected and the output is an images. \n",
    "\n",
    "The training is maybe the most original part. \n",
    "\n",
    "First, you train the discriminator alone by injecting the training set (True data so label=True) and a set of data generated by the generator (label=False) you then have a loss to back propagate. *Applause, you trained you discriminator for one batch*\n",
    "\n",
    "Second, you inject some noise in your generator, the generator produce some fake images, you inject this fake images in you discriminator and use the labels generated by the discriminator to compute your backpropagation. So to train your generator, you will consider the generator and the discriminator as the same network, you just don't train the discriminator in this case.\n",
    "\n",
    "## 12.1\n",
    "\n",
    "We ask you then to program you own GANs. this network should be just compose of fully connected layer (no conv). You should define the architecture of the network, then provide two training function one for the generator and one for the discriminator and make him learn!\n",
    "\n",
    "We will take the MNIST dataset here for simplicity and lightness. The architecture I used was :\n",
    "\n",
    "G : (Input : random Tensor = 100) => (FC1 : fully connected = 256) => (BatchNormalisation) => (LeakyReLU) => (DropOut) => (FC2 : fully connected = 512) => (BatchNormalisation) => (LeakyReLU) => (DropOut) => (FC2 : fully connected = 784) =>  (Tanh)\n",
    "\n",
    "D : (Input : flatten images Tensor = 784) => (FC1 : fully connected = 1024) => (BatchNormalisation) => (LeakyReLU) => (DropOut) => (FC2 : fully connected = 512) => (BatchNormalisation) => (LeakyReLU) => (DropOut) => (FC2 : fully connected = 1) =>  (Sigmoid)\n",
    "\n",
    "This architecture is what I use, it's working but not great, so feel free to change it!!\n",
    "\n",
    "The training is 20-30min on a 1060 6Go for 100 epochs and batch size of 100. Here, it's a little bit different than before, the loss do not necessarly go done, it's a race between D and G so both cannot be good at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "GENERATOR_INPUT_SIZE = 100\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE_G = 0.01\n",
    "LEARNING_RATE_D = 0.01\n",
    "MOMENTUM_G = 0.5\n",
    "MOMENTUM_D = 0.5\n",
    "# n_epochs = 3\n",
    "# batch_size_train = 10\n",
    "# batch_size_test = 1000\n",
    "# log_interval = 10\n",
    "\n",
    "# random_seed = 1\n",
    "# torch.backends.cudnn.enabled = False\n",
    "# torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import save_image\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "import numpy as np\n",
    "\n",
    "# If Cuda\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Image processing\n",
    "transform = transforms.Compose([transforms.ToTensor(), \n",
    "                                transforms.Normalize(mean=(0.5,), std=(0.5,))])\n",
    "\n",
    "# MNIST dataset\n",
    "train_mnist = torchvision.datasets.MNIST(root='./data/',\n",
    "                                   train=True,\n",
    "                                   transform=transform,\n",
    "                                   download=True)\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_mnist,\n",
    "                                          batch_size=BATCH_SIZE, \n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "examples = enumerate(train_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "example_data.shape\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "  plt.subplot(2, 3, i + 1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    D :\n",
    "    (Input : flatten images Tensor = 784)\n",
    "    => (FC1 : fully connected = 1024)\n",
    "    => (BatchNormalisation)\n",
    "    => (LeakyReLU)\n",
    "    => (DropOut)\n",
    "    => (FC2 : fully connected = 512)\n",
    "    => (BatchNormalisation)\n",
    "    => (LeakyReLU)\n",
    "    => (DropOut)\n",
    "    => (FC3 : fully connected = 1)\n",
    "    => (Sigmoid)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(28**2, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(),\n",
    "            # Not more than 25%, http://iamtrask.github.io/2015/07/28/dropout/\n",
    "            nn.Dropout(p=0.2),\n",
    "            \n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=0.4),\n",
    "            \n",
    "            nn.Linear(512, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        print(img.shape)\n",
    "        flattened_img = img.view(img.size(0), -1)\n",
    "        return self.seq(flattened_img)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    G :\n",
    "    (Input : random Tensor = 100)\n",
    "    => (FC1 : fully connected = 256)\n",
    "    => (BatchNormalisation)\n",
    "    => (LeakyReLU)\n",
    "    => (DropOut)\n",
    "    => (FC2 : fully connected = 512)\n",
    "    => (BatchNormalisation)\n",
    "    => (LeakyReLU)\n",
    "    => (DropOut)\n",
    "    => (FC2 : fully connected = 784)\n",
    "    => (Tanh)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(GENERATOR_INPUT_SIZE, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(),\n",
    "            # Not more than 25%, http://iamtrask.github.io/2015/07/28/dropout/\n",
    "            nn.Dropout(p=0.2),\n",
    "            \n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=0.4),\n",
    "            \n",
    "            nn.Linear(512, 784),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.seq(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function:\n",
    "# In the paper, the GAN corresponds to a minimax 2-player game.\n",
    "# As of\n",
    "# https://developers.google.com/machine-learning/gan/loss#minimax-loss\n",
    "# \"Tthe formula derives from the cross-entropy between the real and generated distributions.\"\n",
    "# Thus, we use a cross-entropy loss function pytorch provides:\n",
    "# adversarial_loss = nn.CrossEntropyLoss()\n",
    "# adversarial_loss = nn.BCELoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "G = Generator()\n",
    "D = Discriminator()\n",
    "\n",
    "G.to(device)\n",
    "D.to(device)\n",
    "criterion.to(device)\n",
    "\n",
    "# Optimizers\n",
    "# optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer_G = optim.SGD(G.parameters(), lr=LEARNING_RATE_G, momentum=MOMENTUM_G)\n",
    "optimizer_D = optim.SGD(D.parameters(), lr=LEARNING_RATE_D, momentum=MOMENTUM_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs=30, k=1):\n",
    "    \"\"\"\n",
    "    Algorithm 1:\n",
    "    Minibatch stochastic gradient descent training of generative adversarial nets.\n",
    "    The number of steps to apply to the discriminator, k, is a hyperparameter.\n",
    "    We used k = 1, the least expensive option, in our experiments.\n",
    "    \n",
    "    for number of training iterations do\n",
    "        for k steps do\n",
    "            • Sample minibatch of m noise samples {z(1), ... , z(m)} from noise prior pg(z).\n",
    "            • Sample minibatch of m examples {x(1), ... , x(m)} from data generating distribution pdata (x).\n",
    "            • Update the discriminator by ascending its stochastic gradient\n",
    "        end for\n",
    "        • Sample minibatch of m noise samples {z(1), . . . , z(m)} from noise prior pg(z).\n",
    "        • Update the generator by descending its stochastic gradient\n",
    "    end for\n",
    "\n",
    "    The gradient-based updates can use any standard gradient-based learning rule.\n",
    "    We used momentum in our experiments.\n",
    "    \n",
    "    ======================================================================\n",
    "    \n",
    "    The training is maybe the most original part:\n",
    "    First, you train the discriminator alone by injecting the training set (True data so label=True)\n",
    "    and a set of data generated by the generator (label=False) you then have a loss to back propagate.\n",
    "    Applause, you trained you discriminator for one batch.\n",
    "    Second, you inject some noise in your generator, the generator produce some fake images.\n",
    "    You inject these fake images into your discriminator and use the labels\n",
    "    generated by the discriminator to compute your backpropagation.\n",
    "    So to train your generator, you will consider the generator and\n",
    "    the discriminator as the same network, you just don't train the discriminator in this case.\n",
    "    \n",
    "    \n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        #print(inputs.shape)\n",
    "        #print(labels.shape)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        #print(outputs.shape)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "    \"\"\"\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for step in range(k):\n",
    "            for batch_idx, data in enumerate(train_loader):\n",
    "                images, _labels = data\n",
    "                batch_size = images.shape[0]\n",
    "                labels_real = torch.ones(batch_size, 1, requires_grad=False)\n",
    "                labels_fake = torch.zeros(batch_size, 1, requires_grad=False)\n",
    "                \n",
    "                optimizer_D.zero_grad()\n",
    "                optimizer_G.zero_grad()\n",
    "                \n",
    "                # > First, you train the discriminator alone\n",
    "                # > by injecting the training set (true data so label=True)\n",
    "                output_real = D(images)\n",
    "                # > and a set of data generated by the generator (label=False).\n",
    "                # Create random/noise tensors (z space) as inputs for the generator.\n",
    "                z = torch.randn(batch_size, GENERATOR_INPUT_SIZE)\n",
    "                generated_imgs = G(z)\n",
    "                output_fake = D(generated_imgs)\n",
    "#                 output_fake = D(generated_imgs.detach())\n",
    "                # > You then have a loss to back propagate.\n",
    "                # Measure discriminator's ability to classify real from generated samples\n",
    "                real_loss = criterion(output_real, labels_real)\n",
    "                fake_loss = criterion(output_fake, labels_fake)\n",
    "                # loss_D = (real_loss + fake_loss) / 2\n",
    "                loss_D = real_loss + fake_loss\n",
    "                loss_D.backward()\n",
    "                # > Applause, you trained you discriminator for one batch.\n",
    "                \n",
    "                # > Second, you inject some noise in your generator,\n",
    "                # > the generator produces some fake images.\n",
    "                # > You inject these fake images into your discriminator\n",
    "                # > and use the labels generated by the discriminator\n",
    "                # > to compute your backpropagation.\n",
    "                # > So to train your generator, you will consider the generator and\n",
    "                # > the discriminator as the same network,\n",
    "                # > you just don't train the discriminator in this case.\n",
    "                \n",
    "                \n",
    "                \n",
    "#                 # Sample noise (z space) as input for the generator\n",
    "#                 # z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], opt.latent_dim))))\n",
    "#                 # Generate a batch of images\n",
    "#                 generated_imgs = G(z)\n",
    "                loss_G = criterion(D(generated_imgs), ___)\n",
    "                loss_G.backward()\n",
    "                \n",
    "                optimizer_G.step()\n",
    "                optimizer_D.step()\n",
    "                \n",
    "                print(\n",
    "                    \"[epoch %d/%d] [batch %d/%d] [loss D: %f] [loss G: %f]\"\n",
    "                    % (\n",
    "                        epoch, epochs,\n",
    "                        batch_idx, len(train_loader),\n",
    "                        loss_D.item(),\n",
    "                        loss_G.item(),\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "                # Sample minibatch of m noise samples\n",
    "                # Sample minibatch of m examples\n",
    "                # SGD discriminator w/ momentum\n",
    "\n",
    "            # sample minibatch of m noise samples\n",
    "            # SCD generator w/ momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [2800 x 28], m2: [784 x 1024] at ../aten/src/TH/generic/THTensorMath.cpp:197",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-38b23f53163f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, k)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;31m# > First, you train the discriminator alone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0;31m# > by injecting the training set (true data so label=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                 \u001b[0moutput_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m                 \u001b[0;31m# > and a set of data generated by the generator (label=False).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;31m# Create random/noise tensors (z space) as inputs for the generator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Studium/6 Bachelor Informatik/2019-2020/Computer Vision/assignments/venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-52d985ae18d2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Studium/6 Bachelor Informatik/2019-2020/Computer Vision/assignments/venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Studium/6 Bachelor Informatik/2019-2020/Computer Vision/assignments/venv/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Studium/6 Bachelor Informatik/2019-2020/Computer Vision/assignments/venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Studium/6 Bachelor Informatik/2019-2020/Computer Vision/assignments/venv/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Studium/6 Bachelor Informatik/2019-2020/Computer Vision/assignments/venv/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1372\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1373\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [2800 x 28], m2: [784 x 1024] at ../aten/src/TH/generic/THTensorMath.cpp:197"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Bonus\n",
    "For the bonus, fancy stuff we would like you to program a conditional GAN. This GAN should produce the output you want. Instead of feeding your generator with just noise, you feed it with noise and label and it should generated the number you want. \n",
    "To do so, you add a label part to your Generator's input and you tell your discriminator what number it should recognize by adding a label part to your Discriminator's input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/znxlwm/pytorch-MNIST-CelebA-cGAN-cDCGAN/master/pytorch_cGAN.png\" width=\"900\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://raw.githubusercontent.com/znxlwm/pytorch-MNIST-CelebA-cGAN-cDCGAN/master/pytorch_cGAN.png\", width=900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ToDo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignments",
   "language": "python",
   "name": "assignments"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
